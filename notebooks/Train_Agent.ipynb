{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training the agent"
      ],
      "metadata": {
        "id": "isD2FGz19Q1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ],
      "metadata": {
        "id": "9wugzXUvEKuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# workspace directory \n",
        "WORKSPACE_DIR = \"/content/gdrive/MyDrive/RLF002/vse-002\"\n",
        "\n",
        "# environment parameters\n",
        "race = \"Shanghai_2019\"  # set race (see racesim/input/parameters for possible races)\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available),\n",
        "# 'multi_agent' (if VSE should learn for all drivers at once)\n",
        "vse_others = \"basestrategy\"\n",
        "mcs_pars_file = \"pars_mcs.ini\"  # parameter file for Monte Carlo parameters\n",
        "race_pars_file = f\"pars_{race}.ini\"\n",
        "\n",
        "# hyperparameters\n",
        "num_iterations = 1\n",
        "replay_buffer_max_length = 200_000\n",
        "initial_collect_steps = 200\n",
        "collect_steps_per_iteration = 1\n",
        "\n",
        "fc_layer_params = (64, 64,)\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "gamma = 1.0  # discount rate\n",
        "n_step_update = 1\n",
        "target_update_period = 1\n",
        "dueling_q_net = False\n",
        "\n",
        "# training options\n",
        "num_iterations = 100_000\n",
        "log_interval = 10_000 # 100_000\n",
        "eval_interval = 5_000 # 50_000\n",
        "checkpoint_interval = 5_000\n",
        "num_eval_episodes = 100\n",
        "\n",
        "# postprocessing (currently not implemented for multi-agent environment)\n",
        "calculate_final_positions = False  # activate or deactivate evaluation after training\n",
        "num_races_postproc = 10_000\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available)\n",
        "vse_others_postproc = \"basestrategy\""
      ],
      "metadata": {
        "id": "foVr8R3wD76l"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive."
      ],
      "metadata": {
        "id": "xsAPFt7LCAsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3_vL5uoB-oy",
        "outputId": "654309cc-11d1-40f8-fb58-2d9e2dfb7841"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Colab settings."
      ],
      "metadata": {
        "id": "T3goytPRB8Z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3ioujpmB3fX",
        "outputId": "0173f980-3648-444c-d058-3b8f67d766f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = \"\\n\".join(gpu_info)\n",
        "if gpu_info.find(\"failed\") >= 0:\n",
        "  print(\"Not connected to a GPU\")\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n\")\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print(\"Not using a high-RAM runtime\")\n",
        "else:\n",
        "  print(\"You are using a high-RAM runtime!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "xKLl8tLs9eLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install code repository"
      ],
      "metadata": {
        "id": "7DcaHnmrCH3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/pezon/msca-race-simulation \n",
        "!cp -R msca-race-simulation/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKv7Rl2kCDZJ",
        "outputId": "9f059899-fd42-499d-a735-dd1d5881b10c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'msca-race-simulation'...\n",
            "remote: Enumerating objects: 362, done.\u001b[K\n",
            "remote: Counting objects: 100% (362/362), done.\u001b[K\n",
            "remote: Compressing objects: 100% (301/301), done.\u001b[K\n",
            "remote: Total 362 (delta 176), reused 215 (delta 52), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (362/362), 3.64 MiB | 21.95 MiB/s, done.\n",
            "Resolving deltas: 100% (176/176), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies."
      ],
      "metadata": {
        "id": "N622WipICSdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC95MIfJCQQR",
        "outputId": "e1461ace-518a-474c-836f-4ae836128770"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.20.1)\n",
            "Collecting tf-agents (from -r requirements.txt (line 15))\n",
            "  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (3.2.3)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (0.1.8)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents->-r requirements.txt (line 15))\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from tf-agents->-r requirements.txt (line 15))\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability (from -r requirements.txt (line 14))\n",
            "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 11)) (0.40.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents->-r requirements.txt (line 15)) (0.0.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 11)) (0.1.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy->-r requirements.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697643 sha256=8d00a44e3660c8df1bebefee3f128aeec2f9efd4c1c1a0b5b19f920b4094e1bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf-agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.20.1\n",
            "    Uninstalling tensorflow-probability-0.20.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.20.1\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.19.0 tf-agents-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "OXzDj4Qo9lzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "_EdZK9z_Ckap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.metrics.tf_metrics import AverageReturnMetric\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.policies.py_tf_eager_policy import PyTFEagerPolicy\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer \\\n",
        "  import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tqdm import trange\n",
        "\n",
        "from helper_funcs.src.io import save_preprocessor, save_policy_tflite\n",
        "from machine_learning_rl_training.src.rl_environment_multi_agent \\\n",
        "  import RaceSimulation as MultiAgentRaceSimulation\n",
        "from machine_learning_rl_training.src.rl_environment_single_agent \\\n",
        "  import RaceSimulation as SingleAgentRaceSimulation\n",
        "from racesim.src.import_pars import import_pars\n",
        " \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set working directory\n",
        "workspace_dir = Path(WORKSPACE_DIR)\n",
        "checkpoint_dir = workspace_dir / \"checkpoint\"\n",
        "policy_dir = workspace_dir / \"policy\"\n",
        "export_dir = workspace_dir / \"exports\"\n",
        "\n",
        "# Create directories\n",
        "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
        "policy_dir.mkdir(exist_ok=True, parents=True)\n",
        "export_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")"
      ],
      "metadata": {
        "id": "3e4_IcMbCjqN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check training input."
      ],
      "metadata": {
        "id": "m1gJsHkBCf2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if vse_others == \"multi_agent\" and calculate_final_positions:\n",
        "    print(\"WARNING: Evaluation of trained strategy is currently not implemented for the multi-agent environment!\"\n",
        "          \" Setting calculate_final_positions = False!\")\n",
        "    calculate_final_positions = False\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "# CHECK FOR WET RACE ---------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# load parameter file\n",
        "pars_in = import_pars(\n",
        "    use_print=False,\n",
        "    use_vse=False,\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file)[0]\n",
        "\n",
        "# loop through drivers and check for intermediate or wet tire compounds in real race\n",
        "for driver in pars_in[\"driver_pars\"]:\n",
        "    if any([True if strat[1] in [\"I\", \"W\"] else False for strat in pars_in[\"driver_pars\"][driver][\"strategy_info\"]]):\n",
        "        raise RuntimeError(f\"Cannot train for current race {race} because it was a (partly) wet race!\")"
      ],
      "metadata": {
        "id": "tcKh0ricCUHn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup environment"
      ],
      "metadata": {
        "id": "KEEifSb-C9GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if vse_others == \"multi_agent\":\n",
        "    train_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "    eval_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "else:\n",
        "    train_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "    eval_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "\n",
        "train_tf_env = TFPyEnvironment(environment=train_py_env)\n",
        "eval_tf_env = TFPyEnvironment(environment=eval_py_env)\n",
        "\n",
        "print(f\"INFO: Race: {race}, strategy of other drivers: {vse_others}\")\n",
        "if train_py_env.batched:\n",
        "    print(f\"INFO: Batched environment: {train_py_env.batched()}, batch size: {train_py_env.batch_size}\")\n",
        "print(f\"INFO: Observation spec: {train_py_env.time_step_spec().observation}\")\n",
        "print(f\"INFO: Action spec: {train_py_env.action_spec()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkvmZh8CvHa",
        "outputId": "fef1c947-c8f3-464d-f70e-126fe2879d38"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Race: Shanghai_2019, strategy of other drivers: basestrategy\n",
            "INFO: Observation spec: BoundedArraySpec(shape=(40,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n",
            "INFO: Action spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup DQN Agent"
      ],
      "metadata": {
        "id": "UYYbF0q1DkQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_net = QNetwork(\n",
        "    input_tensor_spec=train_tf_env.observation_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "boltzmann_fn = PolynomialDecay(\n",
        "    initial_learning_rate=1.0,\n",
        "    decay_steps=num_iterations,\n",
        "    end_learning_rate=0.01)\n",
        "\n",
        "agent = DqnAgent(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    n_step_update=n_step_update,\n",
        "    target_update_period=target_update_period,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=global_step)\n",
        "\n",
        "agent.initialize()"
      ],
      "metadata": {
        "id": "J846kRl4Dbuh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup policy"
      ],
      "metadata": {
        "id": "93rbkpEHEU4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = RandomTFPolicy(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec())\n",
        "\n",
        "eager_policy = PyTFEagerPolicy(\n",
        "    agent.collect_policy,\n",
        "    use_tf_function=True)"
      ],
      "metadata": {
        "id": "QwgMb9TeEMJ-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collection\n",
        "\n",
        "We use a Driver to collect experience in an environment. To use a Driver, we specify an observer `replay_buffer.add_batch` that instructs the driver to add trajectory elements to the replay buffer when it receives a trajectory. \n",
        "\n",
        "Then we run the experience collecting loop using the driver.\n",
        "\n",
        "Source: [DynamicStepDriver | TensorFlow Documentation](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver)"
      ],
      "metadata": {
        "id": "NdD8lAxHElqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_tf_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "driver = DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[\n",
        "        replay_buffer.add_batch,\n",
        "        avg_return_metric,\n",
        "    ],\n",
        "    num_steps=collect_steps_per_iteration)\n",
        "\n",
        "# Initial data collection:\n",
        "# initial driver.run will reset the environment and initialize the policy\n",
        "for _ in range(initial_collect_steps):\n",
        "    final_time_step, policy_state = driver.run()\n",
        "print(final_time_step, policy_state)\n",
        "print(avg_return_metric.result().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCwIZeCWElEe",
        "outputId": "e33be1d5-a0a3-4d7c-e0ab-5b5ef42e8bb1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 40), dtype=float32, numpy=\n",
            "array([[0.64285713, 0.8947368 , 0.        , 0.21428572, 0.        ,\n",
            "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.11821312], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}) ()\n",
            "-44.890244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading data for a train step\n",
        "\n",
        "After adding trajectory elements to the replay buffer, we can read batches of trajectory fom the replay buffer to use as input for a train step."
      ],
      "metadata": {
        "id": "4ZziMiUDKHSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset generates trajectories with shape [BxTx...] where\n",
        "# T = n_step_update + 1.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2\n",
        ").prefetch(3)\n",
        "\n",
        "# inspection:\n",
        "dataset_iterator = iter(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWXBSqxrKNv9",
        "outputId": "36f4bd62-5b2e-4617-8f53-06e35f03a136"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.counter(...)` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup checkpointing and saving"
      ],
      "metadata": {
        "id": "CVRHU6bWB7ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.policies.policy_saver import PolicySaver\n",
        "from tf_agents.utils.common import Checkpointer\n",
        "\n",
        "train_checkpointer = Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=20,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")\n",
        "\n",
        "policy_saver = PolicySaver(agent.policy)"
      ],
      "metadata": {
        "id": "kiT1pZ5iB7uM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there is checkpoint saved in the working directory, it will be restored."
      ],
      "metadata": {
        "id": "AKLCaym3Dntf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Restoring checkpoint: {checkpoint_dir}\")\n",
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p4Facv1DYz3",
        "outputId": "b2f99c00-f0bf-48d0-9d87-e0c4b589d8bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restoring checkpoint: /content/gdrive/MyDrive/RLF002/vse-002/checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metrics"
      ],
      "metadata": {
        "id": "NzpeKPNjhsBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
        "\n",
        "eval_avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "eval_driver = DynamicEpisodeDriver(\n",
        "    eval_tf_env,\n",
        "    agent.policy,\n",
        "    observers=[\n",
        "        eval_avg_return_metric,\n",
        "    ],\n",
        "    num_episodes=num_eval_episodes)"
      ],
      "metadata": {
        "id": "76H0rq3Qhtz2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "1. collect data from the environment\n",
        "2. use that data to train the agent's neural network\n",
        "\n",
        "Periodically, we evaluate the policy and print the cur rent score."
      ],
      "metadata": {
        "id": "V53cwz97fNsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# reset training step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# evaluate the agent's policy once before training\n",
        "eval_tf_env.reset()\n",
        "eval_driver.run()\n",
        "rewards = [eval_avg_return_metric.result()]\n",
        "\n",
        "# reset the environment\n",
        "time_step = train_tf_env.reset()\n",
        "\n",
        "for _ in (pbar := trange(num_iterations)):\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    time_step, policy_state = driver.run()\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, _ = next(dataset_iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "    step = int(agent.train_step_counter.numpy())\n",
        "\n",
        "    # Update progress bar status\n",
        "    if step % log_interval == 0:\n",
        "        pbar.set_description(f\"{step=}, {train_loss=:.3f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    if step % eval_interval == 0:\n",
        "        pbar.set_description(f\"Evaluating. {step=}\")\n",
        "        eval_tf_env.reset()\n",
        "        eval_driver.run()\n",
        "        rewards.append(eval_avg_return_metric.result())\n",
        "        pbar.set_description(\n",
        "            f\"{step=}, average return={rewards[-1]:.3f}\")\n",
        "\n",
        "    # Checkpoint / save models\n",
        "    if step % checkpoint_interval == 0:\n",
        "        train_checkpointer.save(global_step)\n",
        "        save_preprocessor(train_py_env, export_dir / f\"{today}-{step}\", race=race)\n",
        "        save_policy_tflite(policy_dir, export_dir / f\"{today}-{step}\", race=race)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W28I1imVfPuZ",
        "outputId": "98a29c36-66ac-4e9b-e27d-6c9e8c30b374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100000 [00:00<?, ?it/s]WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "Evaluating. step=5000:   5%|▍         | 4998/100000 [03:40<2:12:08, 11.98it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "PwukRGyc6MpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint model at the end of training"
      ],
      "metadata": {
        "id": "oho_64R_E7fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_checkpointer.save(global_step)\n",
        "print(f\"Saved checkpoint: {checkpoint_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXFBc-Y9tBTA",
        "outputId": "bb8eb2be-d1ce-4678-c53b-f66c2183323e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint: /content/gdrive/MyDrive/RLF1/vse-1/checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save preprocessor"
      ],
      "metadata": {
        "id": "AFc87Xd_I-s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exported_preprocessor = export_dir / f\"{today}_final\"\n",
        "exported_preprocessor.mkdir()\n",
        "\n",
        "saved_preprocessor = save_preprocessor(train_py_env, exported_preprocessor)\n",
        "print(f\"{saved_preprocessor=}\")"
      ],
      "metadata": {
        "id": "K1fEZ-CwHOEQ"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the policy\n",
        "\n",
        "Converts Q Network to TFlite. See [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) for more details."
      ],
      "metadata": {
        "id": "Ne2sv_pIHKRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_saver.save(policy_dir)\n",
        "print(f\"Saved policy: {policy_dir}\")\n",
        "\n",
        "saved_tflite = save_policy_tflite(policy_dir, exported_preprocessor)\n",
        "print(f\"{saved_tflite=}\")"
      ],
      "metadata": {
        "id": "cP2zuvqAJByj"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the checkpoint and policy zip files."
      ],
      "metadata": {
        "id": "_NurPUmVt1lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download_archive(exported_checkpoint)\n",
        "# download_archive(exported_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8WDF7hRqtsm9",
        "outputId": "fe720ac1-ff2b-4794-be41-58daf36f6cbd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7aeace45-5f2d-4ef7-a13f-9c0d75d3618a\", \"checkpoints.zip\", 100808)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_948f61bb-f4f4-44a8-923c-8f567028a40a\", \"policy-tflite.zip\", 39210)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, you can either (1) continue training iterations, or (2) generate an artifact to check the performance of the loaded policy, or (3) save the policy.\n",
        "\n",
        "When you save the policy and restore it, you cannot continue with the training, but you can deploy the model."
      ],
      "metadata": {
        "id": "44ZGN9qGxOlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "8HxU1frr6RzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from machine_learning_rl_training.src.rl_evaluate_policy import print_returns_positions\n",
        "\n",
        "\n",
        "py_env = SingleAgentRaceSimulation(\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file,\n",
        "    vse_type=vse_others,\n",
        "    use_prob_infl=True,\n",
        "    create_rand_events=True)\n",
        "\n",
        "\n",
        "print_returns_positions(\n",
        "    py_env=py_env,\n",
        "    num_races=num_races_postproc,\n",
        "    tf_lite_path=saved_tflite,\n",
        "    vse_others=vse_others_postproc)"
      ],
      "metadata": {
        "id": "yTvsb-d36SrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}