{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training the agent"
      ],
      "metadata": {
        "id": "isD2FGz19Q1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ],
      "metadata": {
        "id": "9wugzXUvEKuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# workspace directory \n",
        "WORKSPACE_DIR = \"/content/gdrive/MyDrive/RLF002/vse-004-from-basestrat\"\n",
        "\n",
        "# environment parameters\n",
        "# set race (see racesim/input/parameters for possible races)\n",
        "race = \"Shanghai_2019\"\n",
        "race_pars_file = f\"/content/racesim/input/parameters/pars_{race}.ini\"\n",
        "mcs_pars_file = \"/content/racesim/input/parameters/pars_mcs.ini\"\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available),\n",
        "# 'multi_agent' (if VSE should learn for all drivers at once)\n",
        "vse_others = \"basestrategy\"\n",
        "\n",
        "# hyperparameters\n",
        "num_iterations = 1\n",
        "replay_buffer_max_length = 200_000\n",
        "initial_collect_steps = 200\n",
        "collect_steps_per_iteration = 1\n",
        "\n",
        "fc_layer_params = (64, 64,)\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "gamma = 1.0  # discount rate\n",
        "n_step_update = 1\n",
        "target_update_period = 1\n",
        "dueling_q_net = False\n",
        "\n",
        "# training options\n",
        "num_iterations = 5_000 # 100_000\n",
        "log_interval = 5_000 # 100_000\n",
        "eval_interval = 5_000 # 50_000\n",
        "checkpoint_interval = 5_000\n",
        "num_eval_episodes = 100\n",
        "\n",
        "# postprocessing (currently not implemented for multi-agent environment)\n",
        "calculate_final_positions = False  # activate or deactivate evaluation after training\n",
        "num_races_postproc = 10_000\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available)\n",
        "vse_others_postproc = \"basestrategy\"\n",
        "\n",
        "vse_paths = {\n",
        "    \"supervised_nnmodel_cc\": \"/content/racesim/input/vse/nn_supervised_compoundchoice.tflite\",\n",
        "    \"supervised_nnmodel_tc\": \"/content/racesim/input/vse/nn_supervised_tirechange.tflite\",\n",
        "    \"supervised_preprocessor_cc\": \"/content/racesim/input/vse/preprocessor_supervised_compoundchoice.pkl\",\n",
        "    \"supervised_preprocessor_tc\": \"/content/racesim/input/vse/preprocessor_supervised_tirechange.pkl\"\n",
        "}"
      ],
      "metadata": {
        "id": "foVr8R3wD76l"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive."
      ],
      "metadata": {
        "id": "xsAPFt7LCAsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3_vL5uoB-oy",
        "outputId": "1aa14ce7-3d54-4a5e-ca2e-3d2807d7c00f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Colab settings."
      ],
      "metadata": {
        "id": "T3goytPRB8Z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3ioujpmB3fX",
        "outputId": "09b0757e-b1a6-469a-a507-46dc714e3091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = \"\\n\".join(gpu_info)\n",
        "if gpu_info.find(\"failed\") >= 0:\n",
        "  print(\"Not connected to a GPU\")\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n\")\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print(\"Not using a high-RAM runtime\")\n",
        "else:\n",
        "  print(\"You are using a high-RAM runtime!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "xKLl8tLs9eLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install code repository"
      ],
      "metadata": {
        "id": "7DcaHnmrCH3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/pezon/msca-race-simulation \n",
        "!cp -R msca-race-simulation/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKv7Rl2kCDZJ",
        "outputId": "ef8d1093-e5ab-4c70-d8b0-14f31f47b115"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'msca-race-simulation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies."
      ],
      "metadata": {
        "id": "N622WipICSdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC95MIfJCQQR",
        "outputId": "0076bca8-6cff-4f9f-807f-ea2f3decf8e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.19.0)\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (3.2.3)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (0.1.8)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (0.23.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 11)) (0.40.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents->-r requirements.txt (line 15)) (0.0.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 11)) (0.1.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy->-r requirements.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "OXzDj4Qo9lzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "_EdZK9z_Ckap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.metrics.tf_metrics import AverageReturnMetric\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.policies.py_tf_eager_policy import PyTFEagerPolicy\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer \\\n",
        "  import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tqdm import trange\n",
        "\n",
        "from helper_funcs.src.io import save_preprocessor, save_policy_tflite\n",
        "from machine_learning_rl_training.src.rl_environment_multi_agent \\\n",
        "  import RaceSimulation as MultiAgentRaceSimulation\n",
        "from machine_learning_rl_training.src.rl_environment_single_agent \\\n",
        "  import RaceSimulation as SingleAgentRaceSimulation\n",
        "from racesim.src.import_pars import import_pars\n",
        " \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set working directory\n",
        "workspace_dir = Path(WORKSPACE_DIR)\n",
        "checkpoint_dir = workspace_dir / \"checkpoint\"\n",
        "policy_dir = workspace_dir / \"policy\"\n",
        "export_dir = workspace_dir / \"exports\"\n",
        "\n",
        "# Create directories\n",
        "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
        "policy_dir.mkdir(exist_ok=True, parents=True)\n",
        "export_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")"
      ],
      "metadata": {
        "id": "3e4_IcMbCjqN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check training input."
      ],
      "metadata": {
        "id": "m1gJsHkBCf2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "race_pars_file"
      ],
      "metadata": {
        "id": "AyVvgTICpv8t",
        "outputId": "c1166992-24cd-4678-ba4d-ec7f80ed5cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/racesim/input/parameters/pars_Shanghai_2019.ini'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if vse_others == \"multi_agent\" and calculate_final_positions:\n",
        "    print(\"WARNING: Evaluation of trained strategy is currently not implemented for the multi-agent environment!\"\n",
        "          \" Setting calculate_final_positions = False!\")\n",
        "    calculate_final_positions = False\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "# CHECK FOR WET RACE ---------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# load parameter file\n",
        "pars_in = import_pars(\n",
        "    use_print=False,\n",
        "    use_vse=False,\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file)[0]\n",
        "\n",
        "# loop through drivers and check for intermediate or wet tire compounds in real race\n",
        "for driver in pars_in[\"driver_pars\"]:\n",
        "    if any([True if strat[1] in [\"I\", \"W\"] else False for strat in pars_in[\"driver_pars\"][driver][\"strategy_info\"]]):\n",
        "        raise RuntimeError(f\"Cannot train for current race {race} because it was a (partly) wet race!\")"
      ],
      "metadata": {
        "id": "tcKh0ricCUHn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup environment"
      ],
      "metadata": {
        "id": "KEEifSb-C9GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if vse_others == \"multi_agent\":\n",
        "    train_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "    eval_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "else:\n",
        "    train_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "    eval_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "\n",
        "train_tf_env = TFPyEnvironment(environment=train_py_env)\n",
        "eval_tf_env = TFPyEnvironment(environment=eval_py_env)\n",
        "\n",
        "print(f\"INFO: Race: {race}, strategy of other drivers: {vse_others}\")\n",
        "if train_py_env.batched:\n",
        "    print(f\"INFO: Batched environment: {train_py_env.batched()}, batch size: {train_py_env.batch_size}\")\n",
        "print(f\"INFO: Observation spec: {train_py_env.time_step_spec().observation}\")\n",
        "print(f\"INFO: Action spec: {train_py_env.action_spec()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkvmZh8CvHa",
        "outputId": "81103c2d-de30-421c-b298-ba3ab356558e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Race: Shanghai_2019, strategy of other drivers: basestrategy\n",
            "INFO: Observation spec: BoundedArraySpec(shape=(40,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n",
            "INFO: Action spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup DQN Agent"
      ],
      "metadata": {
        "id": "UYYbF0q1DkQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_net = QNetwork(\n",
        "    input_tensor_spec=train_tf_env.observation_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "boltzmann_fn = PolynomialDecay(\n",
        "    initial_learning_rate=1.0,\n",
        "    decay_steps=num_iterations,\n",
        "    end_learning_rate=0.01)\n",
        "\n",
        "agent = DqnAgent(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    n_step_update=n_step_update,\n",
        "    target_update_period=target_update_period,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=global_step)\n",
        "\n",
        "agent.initialize()"
      ],
      "metadata": {
        "id": "J846kRl4Dbuh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup policy"
      ],
      "metadata": {
        "id": "93rbkpEHEU4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = RandomTFPolicy(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec())\n",
        "\n",
        "eager_policy = PyTFEagerPolicy(\n",
        "    agent.collect_policy,\n",
        "    use_tf_function=True)"
      ],
      "metadata": {
        "id": "QwgMb9TeEMJ-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collection\n",
        "\n",
        "We use a Driver to collect experience in an environment. To use a Driver, we specify an observer `replay_buffer.add_batch` that instructs the driver to add trajectory elements to the replay buffer when it receives a trajectory. \n",
        "\n",
        "Then we run the experience collecting loop using the driver.\n",
        "\n",
        "Source: [DynamicStepDriver | TensorFlow Documentation](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver)"
      ],
      "metadata": {
        "id": "NdD8lAxHElqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_tf_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "driver = DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[\n",
        "        replay_buffer.add_batch,\n",
        "        avg_return_metric,\n",
        "    ],\n",
        "    num_steps=collect_steps_per_iteration)\n",
        "\n",
        "# Initial data collection:\n",
        "# initial driver.run will reset the environment and initialize the policy\n",
        "for _ in range(initial_collect_steps):\n",
        "    final_time_step, policy_state = driver.run()\n",
        "print(final_time_step, policy_state)\n",
        "print(avg_return_metric.result().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCwIZeCWElEe",
        "outputId": "97259e71-987b-4046-b77b-9eff903cb1fd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 40), dtype=float32, numpy=\n",
            "array([[0.64285713, 0.94736844, 0.        , 0.01785714, 0.        ,\n",
            "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
            "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
            "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.1367955], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}) ()\n",
            "-119.22599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading data for a train step\n",
        "\n",
        "After adding trajectory elements to the replay buffer, we can read batches of trajectory fom the replay buffer to use as input for a train step."
      ],
      "metadata": {
        "id": "4ZziMiUDKHSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset generates trajectories with shape [BxTx...] where\n",
        "# T = n_step_update + 1.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2\n",
        ").prefetch(3)\n",
        "\n",
        "# inspection:\n",
        "dataset_iterator = iter(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWXBSqxrKNv9",
        "outputId": "fc5b54e9-f6b6-4e69-dc7b-167d4ffa3d5b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.counter(...)` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup checkpointing and saving"
      ],
      "metadata": {
        "id": "CVRHU6bWB7ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.policies.policy_saver import PolicySaver\n",
        "from tf_agents.utils.common import Checkpointer\n",
        "\n",
        "train_checkpointer = Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=20,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")\n",
        "\n",
        "policy_saver = PolicySaver(agent.policy)"
      ],
      "metadata": {
        "id": "kiT1pZ5iB7uM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there is checkpoint saved in the working directory, it will be restored."
      ],
      "metadata": {
        "id": "AKLCaym3Dntf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Restoring checkpoint: {checkpoint_dir}\")\n",
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p4Facv1DYz3",
        "outputId": "43221a40-f93b-44bb-f754-359f82c1048f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restoring checkpoint: /content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metrics"
      ],
      "metadata": {
        "id": "NzpeKPNjhsBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
        "\n",
        "eval_avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "eval_driver = DynamicEpisodeDriver(\n",
        "    eval_tf_env,\n",
        "    agent.policy,\n",
        "    observers=[\n",
        "        eval_avg_return_metric,\n",
        "    ],\n",
        "    num_episodes=num_eval_episodes)"
      ],
      "metadata": {
        "id": "76H0rq3Qhtz2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "1. collect data from the environment\n",
        "2. use that data to train the agent's neural network\n",
        "\n",
        "Periodically, we evaluate the policy and print the cur rent score."
      ],
      "metadata": {
        "id": "V53cwz97fNsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# reset training step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# evaluate the agent's policy once before training\n",
        "eval_tf_env.reset()\n",
        "eval_driver.run()\n",
        "rewards = [eval_avg_return_metric.result()]\n",
        "\n",
        "# reset the environment\n",
        "time_step = train_tf_env.reset()\n",
        "\n",
        "for _ in (pbar := trange(num_iterations)):\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    time_step, policy_state = driver.run()\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, _ = next(dataset_iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "    step = int(agent.train_step_counter.numpy())\n",
        "\n",
        "    # Update progress bar status\n",
        "    if step % log_interval == 0:\n",
        "        pbar.set_description(f\"{step=}, {train_loss=:.3f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    if step % eval_interval == 0:\n",
        "        pbar.set_description(f\"Evaluating. {step=}\")\n",
        "        eval_tf_env.reset()\n",
        "        eval_driver.run()\n",
        "        rewards.append(eval_avg_return_metric.result())\n",
        "        pbar.set_description(\n",
        "            f\"{step=}, average return={rewards[-1]:.3f}\")\n",
        "\n",
        "    # Checkpoint / save models\n",
        "    if step % checkpoint_interval == 0:\n",
        "        train_checkpointer.save(global_step)\n",
        "        policy_saver.save(policy_dir)\n",
        "        save_preprocessor(train_py_env, export_dir / f\"{today}-{step}\", race=race)\n",
        "        save_policy_tflite(policy_dir, export_dir / f\"{today}-{step}\", race=race)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W28I1imVfPuZ",
        "outputId": "ab80ed04-a647-478c-8130-b11a2e22e3e4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5000 [00:00<?, ?it/s]WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "step=5000, average return=-5.418: 100%|█████████▉| 4997/5000 [04:52<00:00, 33.28it/s]WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n",
            "step=5000, average return=-5.418: 100%|██████████| 5000/5000 [04:54<00:00, 16.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6min 38s, sys: 3.88 s, total: 6min 42s\n",
            "Wall time: 6min 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the agent"
      ],
      "metadata": {
        "id": "CxzrcjykuoLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rewards at evaluation points:"
      ],
      "metadata": {
        "id": "K_7ujmw6vuhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards"
      ],
      "metadata": {
        "id": "BHeVYCMpvsAj",
        "outputId": "0bdb838a-8a23-4015-e649-0b7dfc77a85e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=-125.97166>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=-5.418427>]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a few episodes using learned agent policy. (equivalent to eval_driver metrics)"
      ],
      "metadata": {
        "id": "6232laOEwJep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_funcs.src.io import evaluate_policy\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_tf_env,\n",
        "    eval_py_env,\n",
        "    agent.policy,\n",
        "    num_episodes=3\n",
        "    # num_eval_episodes\n",
        ")"
      ],
      "metadata": {
        "id": "vjNa7jTnutVA",
        "outputId": "2aa9bfa9-2219-402c-b07d-760b2964da55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "race 1: driver = 3, lap = 1, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 2, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 3, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 4, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 5, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 6, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 7, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 8, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 9, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 10, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 11, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 12, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 13, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 14, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 15, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 16, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 17, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 18, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 19, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 20, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 21, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 22, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 23, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 24, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 25, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 26, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 27, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 28, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 29, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 30, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 31, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 32, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 33, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 34, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 35, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 36, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 37, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 38, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 39, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 40, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 41, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 42, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 43, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 44, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 45, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 46, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 47, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 48, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 49, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 50, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 51, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 52, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 53, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 54, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, lap = 55, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 3, final_position = 19\n",
            "race 2: driver = 9, lap = 1, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 2, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 3, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 4, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 5, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 6, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 7, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 8, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 9, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 10, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 11, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 12, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 13, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 14, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 15, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 16, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 17, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 18, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 19, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 20, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 21, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 22, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 23, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 24, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 25, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 26, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 27, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 28, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 29, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 30, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 31, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 32, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 33, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 34, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 35, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 36, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 37, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 38, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 39, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 40, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 41, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 42, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 43, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 44, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 45, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 46, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 47, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 48, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 49, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 50, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 51, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 52, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 53, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 54, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, lap = 55, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 9, final_position = 17\n",
            "race 3: driver = 17, lap = 1, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 2, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 3, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 4, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 5, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 6, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 7, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 8, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 9, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 10, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 11, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 12, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 13, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 14, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 15, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 16, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 17, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 18, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 19, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 20, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 21, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 22, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 23, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 24, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 25, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 26, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 27, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 28, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 29, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 30, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 31, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 32, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 33, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 34, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 35, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 36, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 37, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 38, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 39, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 40, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 41, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 42, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 43, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 44, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 45, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 46, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 47, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 48, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 49, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 50, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 51, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 52, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 53, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 54, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, lap = 55, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 17, final_position = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.632820688406355"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More precise estimate."
      ],
      "metadata": {
        "id": "Kb1MdIV7yWmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_policy(\n",
        "    eval_tf_env,\n",
        "    eval_py_env,\n",
        "    agent.policy,\n",
        "    num_episodes=3,\n",
        "    print_lap_decisions=num_eval_episodes,\n",
        ")"
      ],
      "metadata": {
        "id": "zdeDBXVryWFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "PwukRGyc6MpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint model at the end of training"
      ],
      "metadata": {
        "id": "oho_64R_E7fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_checkpointer.save(global_step)\n",
        "print(f\"Saved checkpoint: {checkpoint_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXFBc-Y9tBTA",
        "outputId": "79a1ecb6-7ac5-48fa-da26-56ff98c44c97"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint: /content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save preprocessor"
      ],
      "metadata": {
        "id": "AFc87Xd_I-s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_preprocessor = save_preprocessor(train_py_env, export_dir / f\"{today}-final\", race=race)\n",
        "print(f\"{saved_preprocessor=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1fEZ-CwHOEQ",
        "outputId": "2d5cdf9f-3891-4c29-976a-5f4d24794625"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_preprocessor=PosixPath('/content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/exports/2023-05-22-final/preprocessor_reinforcement_Shanghai_2019.pkl')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the policy\n",
        "\n",
        "Converts Q Network to TFlite. See [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) for more details."
      ],
      "metadata": {
        "id": "Ne2sv_pIHKRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_saver.save(policy_dir)\n",
        "print(f\"Saved policy: {policy_dir}\")\n",
        "\n",
        "saved_tflite = save_policy_tflite(policy_dir, export_dir / f\"{today}-final\", race=race)\n",
        "print(f\"{saved_tflite=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP2zuvqAJByj",
        "outputId": "874385e1-42c0-42a8-c5b4-bb3c35d76818"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved policy: /content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/policy\n",
            "saved_tflite=PosixPath('/content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/exports/2023-05-22-final/nn_reinforcement_Shanghai_2019.tflite')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the checkpoint and policy zip files."
      ],
      "metadata": {
        "id": "_NurPUmVt1lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download_archive(exported_checkpoint)\n",
        "# download_archive(exported_policy)"
      ],
      "metadata": {
        "id": "8WDF7hRqtsm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, you can either (1) continue training iterations, or (2) generate an artifact to check the performance of the loaded policy, or (3) save the policy.\n",
        "\n",
        "When you save the policy and restore it, you cannot continue with the training, but you can deploy the model."
      ],
      "metadata": {
        "id": "44ZGN9qGxOlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate TFLite model"
      ],
      "metadata": {
        "id": "8HxU1frr6RzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create environment and run a few races."
      ],
      "metadata": {
        "id": "XDgDq5t_y4BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from machine_learning_rl_training.src.rl_evaluate_policy import print_returns_positions\n",
        "\n",
        "py_env = SingleAgentRaceSimulation(\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file,\n",
        "    vse_type=vse_others,\n",
        "    use_prob_infl=True,\n",
        "    create_rand_events=True,\n",
        "    vse_paths=vse_paths)\n",
        "\n",
        "\n",
        "pprint_returns_positions(\n",
        "    py_env=py_env,\n",
        "    num_races=3,\n",
        "    tf_lite_path=str(saved_tflite),\n",
        "    vse_others=vse_others_postproc,\n",
        "    print_lap_decisions=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "yTvsb-d36SrZ",
        "outputId": "2434f9c6-af1a-425e-a849-6139f7dbc5b4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Evaluating reinforcement VSE by average returns and positions over 10000 races against basestrategy VSE...\n",
            "race 1: driver = 2, lap = 1, action = [0]\n",
            "race 1: driver = 2, lap = 2, action = [0]\n",
            "race 1: driver = 2, lap = 3, action = [0]\n",
            "race 1: driver = 2, lap = 4, action = [0]\n",
            "race 1: driver = 2, lap = 5, action = [0]\n",
            "race 1: driver = 2, lap = 6, action = [0]\n",
            "race 1: driver = 2, lap = 7, action = [0]\n",
            "race 1: driver = 2, lap = 8, action = [0]\n",
            "race 1: driver = 2, lap = 9, action = [0]\n",
            "race 1: driver = 2, lap = 10, action = [0]\n",
            "race 1: driver = 2, lap = 11, action = [0]\n",
            "race 1: driver = 2, lap = 12, action = [1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-74cdc2b8909e>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m print_returns_positions(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpy_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpy_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnum_races\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_races_postproc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/machine_learning_rl_training/src/rl_evaluate_policy.py\u001b[0m in \u001b[0;36mprint_returns_positions\u001b[0;34m(py_env, num_races, tf_lite_path, vse_others, tf_lite_version, print_lap_decisions)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"race {i + 1}: driver = {py_env.idx_driver}, lap = {lap}, action = {action_q}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mlap\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/machine_learning_rl_training/src/rl_environment_single_agent.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;31m# pit stop in this lap, compound = available_compounds[action - 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrivers_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_driver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                 \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_lap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_compounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;31m# simulate next lap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a more precise estimate"
      ],
      "metadata": {
        "id": "7sPMapiwy8DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_returns_positions(\n",
        "    py_env=py_env,\n",
        "    num_races=num_races_postproc,\n",
        "    tf_lite_path=str(saved_tflite),\n",
        "    vse_others=vse_others_postproc,\n",
        "    print_lap_decisions=False,\n",
        ")"
      ],
      "metadata": {
        "id": "xyKqT8Iuy1Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7br4f5DxCdx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}