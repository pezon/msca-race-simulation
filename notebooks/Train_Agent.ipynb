{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "isD2FGz19Q1w"
      },
      "source": [
        "# Training the agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9wugzXUvEKuO"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "foVr8R3wD76l"
      },
      "outputs": [],
      "source": [
        "# workspace directory \n",
        "WORKSPACE_DIR = \"/content/gdrive/MyDrive/RLF002/vse-004-from-basestrat\"\n",
        "\n",
        "# environment parameters\n",
        "# set race (see racesim/input/parameters for possible races)\n",
        "race = \"Shanghai_2019\"\n",
        "race_pars_file = f\"/content/racesim/input/parameters/pars_{race}.ini\"\n",
        "mcs_pars_file = \"/content/racesim/input/parameters/pars_mcs.ini\"\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available),\n",
        "# 'multi_agent' (if VSE should learn for all drivers at once)\n",
        "vse_others = \"realstrategy\"\n",
        "\n",
        "# hyperparameters\n",
        "num_iterations = 1\n",
        "replay_buffer_max_length = 200_000\n",
        "initial_collect_steps = 200\n",
        "collect_steps_per_iteration = 1\n",
        "\n",
        "fc_layer_params = (64, 64,)\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "gamma = 1.0  # discount rate\n",
        "n_step_update = 1\n",
        "target_update_period = 1\n",
        "dueling_q_net = False\n",
        "\n",
        "# training options\n",
        "num_iterations      = 5_000   # 100_000\n",
        "log_interval        = 5_000   # 100_000\n",
        "eval_interval       = 100     # 50_000\n",
        "checkpoint_interval = 5_000\n",
        "num_eval_episodes   = 10\n",
        "\n",
        "# postprocessing (currently not implemented for multi-agent environment)\n",
        "calculate_final_positions = False  # activate or deactivate evaluation after training\n",
        "num_races_postproc = 1000  # 10_000\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available)\n",
        "vse_others_postproc = \"realstrategy\"\n",
        "\n",
        "vse_paths = {\n",
        "    # \"reinf_nnmodel\": \"/content/gdrive/MyDrive/RLF000/vse-003-from-reinforcement/exports/2023-05-24-final/nn_reinforcement_Shanghai_2019.tflite\",\n",
        "    # \"reinf_preprocessor\": \"/content/gdrive/MyDrive/RLF000/vse-003-from-realstrat/exports/2023-05-24-final/preprocessor_reinforcement_Shanghai_2019.pkl\",\n",
        "    \"supervised_nnmodel_cc\": \"/content/racesim/input/vse/nn_supervised_compoundchoice.tflite\",\n",
        "    \"supervised_nnmodel_tc\": \"/content/racesim/input/vse/nn_supervised_tirechange.tflite\",\n",
        "    \"supervised_preprocessor_cc\": \"/content/racesim/input/vse/preprocessor_supervised_compoundchoice.pkl\",\n",
        "    \"supervised_preprocessor_tc\": \"/content/racesim/input/vse/preprocessor_supervised_tirechange.pkl\"\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xsAPFt7LCAsp"
      },
      "source": [
        "Mount Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3_vL5uoB-oy",
        "outputId": "416114b1-e912-4ebc-db38-effcd998dee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T3goytPRB8Z7"
      },
      "source": [
        "Check Colab settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3ioujpmB3fX",
        "outputId": "64fbc5df-7652-4a49-873b-dbf7ec9c7534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = \"\\n\".join(gpu_info)\n",
        "if gpu_info.find(\"failed\") >= 0:\n",
        "  print(\"Not connected to a GPU\")\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n\")\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print(\"Not using a high-RAM runtime\")\n",
        "else:\n",
        "  print(\"You are using a high-RAM runtime!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xKLl8tLs9eLt"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7DcaHnmrCH3A"
      },
      "source": [
        "Install code repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKv7Rl2kCDZJ",
        "outputId": "2100fda1-43ce-4846-e731-9443c66e762d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'msca-race-simulation'...\n",
            "remote: Enumerating objects: 365, done.\u001b[K\n",
            "remote: Counting objects: 100% (365/365), done.\u001b[K\n",
            "remote: Compressing objects: 100% (304/304), done.\u001b[K\n",
            "remote: Total 365 (delta 175), reused 209 (delta 52), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (365/365), 4.14 MiB | 18.85 MiB/s, done.\n",
            "Resolving deltas: 100% (175/175), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf msca-race-simulation/\n",
        "!git clone --depth 1 https://github.com/pezon/msca-race-simulation \n",
        "!cp -R msca-race-simulation/* ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N622WipICSdQ"
      },
      "source": [
        "Install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC95MIfJCQQR",
        "outputId": "2cb41319-ebcb-4b6a-806a-64a45023ca94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.20.1)\n",
            "Collecting tf-agents (from -r requirements.txt (line 15))\n",
            "  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (3.2.3)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (0.1.8)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents->-r requirements.txt (line 15))\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from tf-agents->-r requirements.txt (line 15))\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability (from -r requirements.txt (line 14))\n",
            "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 11)) (0.40.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents->-r requirements.txt (line 15)) (0.0.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 11)) (0.1.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy->-r requirements.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697640 sha256=c5793e394877624044dc21d56017b1053969dbb81d1093d329f5f1f333554084\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf-agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.20.1\n",
            "    Uninstalling tensorflow-probability-0.20.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.20.1\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.19.0 tf-agents-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OXzDj4Qo9lzT"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_EdZK9z_Ckap"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3e4_IcMbCjqN"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.metrics.tf_metrics import AverageReturnMetric\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.policies.py_tf_eager_policy import PyTFEagerPolicy\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer \\\n",
        "  import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tqdm import trange\n",
        "\n",
        "from helper_funcs.src.io import save_preprocessor, save_policy_tflite\n",
        "from machine_learning_rl_training.src.rl_environment_multi_agent \\\n",
        "  import RaceSimulation as MultiAgentRaceSimulation\n",
        "from machine_learning_rl_training.src.rl_environment_single_agent \\\n",
        "  import RaceSimulation as SingleAgentRaceSimulation\n",
        "from racesim.src.import_pars import import_pars\n",
        " \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set working directory\n",
        "workspace_dir = Path(WORKSPACE_DIR)\n",
        "checkpoint_dir = workspace_dir / \"checkpoint\"\n",
        "policy_dir = workspace_dir / \"policy\"\n",
        "export_dir = workspace_dir / \"exports\"\n",
        "\n",
        "# Create directories\n",
        "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
        "policy_dir.mkdir(exist_ok=True, parents=True)\n",
        "export_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m1gJsHkBCf2L"
      },
      "source": [
        "### Check parameters \n",
        "Check training input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AyVvgTICpv8t",
        "outputId": "0329fce9-5ccb-4a21-8dc8-0a94e188fea5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/racesim/input/parameters/pars_Shanghai_2019.ini'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "race_pars_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tcKh0ricCUHn"
      },
      "outputs": [],
      "source": [
        "if vse_others == \"multi_agent\" and calculate_final_positions:\n",
        "    print(\"WARNING: Evaluation of trained strategy is currently not implemented for the multi-agent environment!\"\n",
        "          \" Setting calculate_final_positions = False!\")\n",
        "    calculate_final_positions = False\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "# CHECK FOR WET RACE ---------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# load parameter file\n",
        "pars_in = import_pars(\n",
        "    use_print=False,\n",
        "    use_vse=False,\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file)[0]\n",
        "\n",
        "# loop through drivers and check for intermediate or wet tire compounds in real race\n",
        "for driver in pars_in[\"driver_pars\"]:\n",
        "    if any([True if strat[1] in [\"I\", \"W\"] else False for strat in pars_in[\"driver_pars\"][driver][\"strategy_info\"]]):\n",
        "        raise RuntimeError(f\"Cannot train for current race {race} because it was a (partly) wet race!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KEEifSb-C9GA"
      },
      "source": [
        "### Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkvmZh8CvHa",
        "outputId": "dcd64776-8086-4027-ae4d-4cf9bbce4b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Race: Shanghai_2019, strategy of other drivers: basestrategy\n",
            "INFO: Observation spec: BoundedArraySpec(shape=(40,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n",
            "INFO: Action spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3)\n"
          ]
        }
      ],
      "source": [
        "if vse_others == \"multi_agent\":\n",
        "    train_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "    eval_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "else:\n",
        "    train_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "    eval_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True,\n",
        "        vse_paths=vse_paths)\n",
        "\n",
        "train_tf_env = TFPyEnvironment(environment=train_py_env)\n",
        "eval_tf_env = TFPyEnvironment(environment=eval_py_env)\n",
        "\n",
        "print(f\"INFO: Race: {race}, strategy of other drivers: {vse_others}\")\n",
        "if train_py_env.batched:\n",
        "    print(f\"INFO: Batched environment: {train_py_env.batched()}, batch size: {train_py_env.batch_size}\")\n",
        "print(f\"INFO: Observation spec: {train_py_env.time_step_spec().observation}\")\n",
        "print(f\"INFO: Action spec: {train_py_env.action_spec()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UYYbF0q1DkQy"
      },
      "source": [
        "### Setup DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J846kRl4Dbuh"
      },
      "outputs": [],
      "source": [
        "q_net = QNetwork(\n",
        "    input_tensor_spec=train_tf_env.observation_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "boltzmann_fn = PolynomialDecay(\n",
        "    initial_learning_rate=1.0,\n",
        "    decay_steps=num_iterations,\n",
        "    end_learning_rate=0.01)\n",
        "\n",
        "agent = DqnAgent(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    n_step_update=n_step_update,\n",
        "    target_update_period=target_update_period,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=global_step)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "93rbkpEHEU4g"
      },
      "source": [
        "### Setup policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QwgMb9TeEMJ-"
      },
      "outputs": [],
      "source": [
        "random_policy = RandomTFPolicy(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec())\n",
        "\n",
        "eager_policy = PyTFEagerPolicy(\n",
        "    agent.collect_policy,\n",
        "    use_tf_function=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NdD8lAxHElqd"
      },
      "source": [
        "### Collect data\n",
        "\n",
        "We use a Driver to collect experience in an environment. To use a Driver, we specify an observer `replay_buffer.add_batch` that instructs the driver to add trajectory elements to the replay buffer when it receives a trajectory. \n",
        "\n",
        "Then we run the experience collecting loop using the driver.\n",
        "\n",
        "Source: [DynamicStepDriver | TensorFlow Documentation](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCwIZeCWElEe",
        "outputId": "17d8e060-3259-42ab-9b85-46672564d255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 40), dtype=float32, numpy=\n",
            "array([[0.64285713, 0.94736844, 0.        , 0.10714286, 1.        ,\n",
            "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
            "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.20745972], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}) ()\n",
            "-61.376194\n"
          ]
        }
      ],
      "source": [
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_tf_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "driver = DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[\n",
        "        replay_buffer.add_batch,\n",
        "        avg_return_metric,\n",
        "    ],\n",
        "    num_steps=collect_steps_per_iteration)\n",
        "\n",
        "# Initial data collection:\n",
        "# initial driver.run will reset the environment and initialize the policy\n",
        "for _ in range(initial_collect_steps):\n",
        "    final_time_step, policy_state = driver.run()\n",
        "print(final_time_step, policy_state)\n",
        "print(avg_return_metric.result().numpy())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZziMiUDKHSM"
      },
      "source": [
        "Reading data for a train step\n",
        "\n",
        "After adding trajectory elements to the replay buffer, we can read batches of trajectory fom the replay buffer to use as input for a train step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWXBSqxrKNv9",
        "outputId": "6f2b0fae-42f2-4723-bc35-7a076f22a1af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.counter(...)` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "# Dataset generates trajectories with shape [BxTx...] where\n",
        "# T = n_step_update + 1.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2\n",
        ").prefetch(3)\n",
        "\n",
        "# inspection:\n",
        "dataset_iterator = iter(dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CVRHU6bWB7ba"
      },
      "source": [
        "### Setup checkpointing and saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kiT1pZ5iB7uM"
      },
      "outputs": [],
      "source": [
        "from tf_agents.policies.policy_saver import PolicySaver\n",
        "from tf_agents.utils.common import Checkpointer\n",
        "\n",
        "train_checkpointer = Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=20,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")\n",
        "\n",
        "policy_saver = PolicySaver(agent.policy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AKLCaym3Dntf"
      },
      "source": [
        "If there is checkpoint saved in the working directory, it will be restored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p4Facv1DYz3",
        "outputId": "629de856-9ec5-43e9-e954-02b1dd1b38c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restoring checkpoint: /content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/checkpoint\n"
          ]
        }
      ],
      "source": [
        "print(f\"Restoring checkpoint: {checkpoint_dir}\")\n",
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NzpeKPNjhsBD"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "Agent earns `+5` reward per `+1` position changes and `-5` per `-1` position change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "76H0rq3Qhtz2"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
        "\n",
        "eval_avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "eval_driver = DynamicEpisodeDriver(\n",
        "    eval_tf_env,\n",
        "    agent.policy,\n",
        "    observers=[\n",
        "        eval_avg_return_metric,\n",
        "    ],\n",
        "    num_episodes=num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6N-ze861ev9",
        "outputId": "31f35025-66ce-43f0-90d4-2b8140561e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "race 1: driver = 4, lap = 1, action = 2\n",
            "race 1: driver = 4, lap = 2, action = 0\n",
            "race 1: driver = 4, lap = 3, action = 0\n",
            "race 1: driver = 4, lap = 4, action = 0\n",
            "race 1: driver = 4, lap = 5, action = 0\n",
            "race 1: driver = 4, lap = 6, action = 0\n",
            "race 1: driver = 4, lap = 7, action = 0\n",
            "race 1: driver = 4, lap = 8, action = 0\n",
            "race 1: driver = 4, lap = 9, action = 0\n",
            "race 1: driver = 4, lap = 10, action = 0\n",
            "race 1: driver = 4, lap = 11, action = 0\n",
            "race 1: driver = 4, lap = 12, action = 0\n",
            "race 1: driver = 4, lap = 13, action = 0\n",
            "race 1: driver = 4, lap = 14, action = 0\n",
            "race 1: driver = 4, lap = 15, action = 0\n",
            "race 1: driver = 4, lap = 16, action = 0\n",
            "race 1: driver = 4, lap = 17, action = 0\n",
            "race 1: driver = 4, lap = 18, action = 0\n",
            "race 1: driver = 4, lap = 19, action = 0\n",
            "race 1: driver = 4, lap = 20, action = 0\n",
            "race 1: driver = 4, lap = 21, action = 0\n",
            "race 1: driver = 4, lap = 22, action = 0\n",
            "race 1: driver = 4, lap = 23, action = 0\n",
            "race 1: driver = 4, lap = 24, action = 0\n",
            "race 1: driver = 4, lap = 25, action = 0\n",
            "race 1: driver = 4, lap = 26, action = 0\n",
            "race 1: driver = 4, lap = 27, action = 0\n",
            "race 1: driver = 4, lap = 28, action = 0\n",
            "race 1: driver = 4, lap = 29, action = 0\n",
            "race 1: driver = 4, lap = 30, action = 0\n",
            "race 1: driver = 4, lap = 31, action = 0\n",
            "race 1: driver = 4, lap = 32, action = 0\n",
            "race 1: driver = 4, lap = 33, action = 0\n",
            "race 1: driver = 4, lap = 34, action = 0\n",
            "race 1: driver = 4, lap = 35, action = 0\n",
            "race 1: driver = 4, lap = 36, action = 0\n",
            "race 1: driver = 4, lap = 37, action = 0\n",
            "race 1: driver = 4, lap = 38, action = 0\n",
            "race 1: driver = 4, lap = 39, action = 0\n",
            "race 1: driver = 4, lap = 40, action = 0\n",
            "race 1: driver = 4, lap = 41, action = 2\n",
            "race 1: driver = 4, lap = 42, action = 0\n",
            "race 1: driver = 4, lap = 43, action = 0\n",
            "race 1: driver = 4, lap = 44, action = 0\n",
            "race 1: driver = 4, lap = 45, action = 0\n",
            "race 1: driver = 4, lap = 46, action = 0\n",
            "race 1: driver = 4, lap = 47, action = 0\n",
            "race 1: driver = 4, lap = 48, action = 0\n",
            "race 1: driver = 4, lap = 49, action = 0\n",
            "race 1: driver = 4, lap = 50, action = 0\n",
            "race 1: driver = 4, lap = 51, action = 0\n",
            "race 1: driver = 4, lap = 52, action = 0\n",
            "race 1: driver = 4, lap = 53, action = 0\n",
            "race 1: driver = 4, lap = 54, action = 0\n",
            "race 1: driver = 4, lap = 55, action = 0\n",
            "race 1: driver = 4, final_position = 14\n",
            "race 2: driver = 15, lap = 1, action = 0\n",
            "race 2: driver = 15, lap = 2, action = 0\n",
            "race 2: driver = 15, lap = 3, action = 0\n",
            "race 2: driver = 15, lap = 4, action = 0\n",
            "race 2: driver = 15, lap = 5, action = 0\n",
            "race 2: driver = 15, lap = 6, action = 0\n",
            "race 2: driver = 15, lap = 7, action = 0\n",
            "race 2: driver = 15, lap = 8, action = 0\n",
            "race 2: driver = 15, lap = 9, action = 0\n",
            "race 2: driver = 15, lap = 10, action = 0\n",
            "race 2: driver = 15, lap = 11, action = 0\n",
            "race 2: driver = 15, lap = 12, action = 0\n",
            "race 2: driver = 15, lap = 13, action = 0\n",
            "race 2: driver = 15, lap = 14, action = 0\n",
            "race 2: driver = 15, lap = 15, action = 0\n",
            "race 2: driver = 15, lap = 16, action = 0\n",
            "race 2: driver = 15, lap = 17, action = 0\n",
            "race 2: driver = 15, lap = 18, action = 0\n",
            "race 2: driver = 15, lap = 19, action = 0\n",
            "race 2: driver = 15, lap = 20, action = 1\n",
            "race 2: driver = 15, lap = 21, action = 0\n",
            "race 2: driver = 15, lap = 22, action = 0\n",
            "race 2: driver = 15, lap = 23, action = 0\n",
            "race 2: driver = 15, lap = 24, action = 0\n",
            "race 2: driver = 15, lap = 25, action = 0\n",
            "race 2: driver = 15, lap = 26, action = 0\n",
            "race 2: driver = 15, lap = 27, action = 0\n",
            "race 2: driver = 15, lap = 28, action = 0\n",
            "race 2: driver = 15, lap = 29, action = 0\n",
            "race 2: driver = 15, lap = 30, action = 0\n",
            "race 2: driver = 15, lap = 31, action = 0\n",
            "race 2: driver = 15, lap = 32, action = 0\n",
            "race 2: driver = 15, lap = 33, action = 0\n",
            "race 2: driver = 15, lap = 34, action = 0\n",
            "race 2: driver = 15, lap = 35, action = 0\n",
            "race 2: driver = 15, lap = 36, action = 0\n",
            "race 2: driver = 15, lap = 37, action = 0\n",
            "race 2: driver = 15, lap = 38, action = 0\n",
            "race 2: driver = 15, lap = 39, action = 0\n",
            "race 2: driver = 15, lap = 40, action = 0\n",
            "race 2: driver = 15, lap = 41, action = 0\n",
            "race 2: driver = 15, lap = 42, action = 0\n",
            "race 2: driver = 15, lap = 43, action = 0\n",
            "race 2: driver = 15, lap = 44, action = 0\n",
            "race 2: driver = 15, lap = 45, action = 0\n",
            "race 2: driver = 15, lap = 46, action = 0\n",
            "race 2: driver = 15, lap = 47, action = 0\n",
            "race 2: driver = 15, lap = 48, action = 0\n",
            "race 2: driver = 15, lap = 49, action = 0\n",
            "race 2: driver = 15, lap = 50, action = 0\n",
            "race 2: driver = 15, lap = 51, action = 0\n",
            "race 2: driver = 15, lap = 52, action = 0\n",
            "race 2: driver = 15, lap = 53, action = 0\n",
            "race 2: driver = 15, lap = 54, action = 0\n",
            "race 2: driver = 15, lap = 55, action = 0\n",
            "race 2: driver = 15, final_position = 5\n",
            "race 3: driver = 6, lap = 1, action = 0\n",
            "race 3: driver = 6, lap = 2, action = 0\n",
            "race 3: driver = 6, lap = 3, action = 0\n",
            "race 3: driver = 6, lap = 4, action = 0\n",
            "race 3: driver = 6, lap = 5, action = 0\n",
            "race 3: driver = 6, lap = 6, action = 0\n",
            "race 3: driver = 6, lap = 7, action = 0\n",
            "race 3: driver = 6, lap = 8, action = 0\n",
            "race 3: driver = 6, lap = 9, action = 0\n",
            "race 3: driver = 6, lap = 10, action = 0\n",
            "race 3: driver = 6, lap = 11, action = 0\n",
            "race 3: driver = 6, lap = 12, action = 0\n",
            "race 3: driver = 6, lap = 13, action = 0\n",
            "race 3: driver = 6, lap = 14, action = 0\n",
            "race 3: driver = 6, lap = 15, action = 0\n",
            "race 3: driver = 6, lap = 16, action = 0\n",
            "race 3: driver = 6, lap = 17, action = 0\n",
            "race 3: driver = 6, lap = 18, action = 0\n",
            "race 3: driver = 6, lap = 19, action = 0\n",
            "race 3: driver = 6, lap = 20, action = 0\n",
            "race 3: driver = 6, lap = 21, action = 0\n",
            "race 3: driver = 6, lap = 22, action = 0\n",
            "race 3: driver = 6, lap = 23, action = 0\n",
            "race 3: driver = 6, lap = 24, action = 0\n",
            "race 3: driver = 6, lap = 25, action = 0\n",
            "race 3: driver = 6, lap = 26, action = 0\n",
            "race 3: driver = 6, lap = 27, action = 0\n",
            "race 3: driver = 6, lap = 28, action = 0\n",
            "race 3: driver = 6, lap = 29, action = 0\n",
            "race 3: driver = 6, lap = 30, action = 0\n",
            "race 3: driver = 6, lap = 31, action = 0\n",
            "race 3: driver = 6, lap = 32, action = 0\n",
            "race 3: driver = 6, lap = 33, action = 0\n",
            "race 3: driver = 6, lap = 34, action = 0\n",
            "race 3: driver = 6, lap = 35, action = 0\n",
            "race 3: driver = 6, lap = 36, action = 0\n",
            "race 3: driver = 6, lap = 37, action = 0\n",
            "race 3: driver = 6, lap = 38, action = 0\n",
            "race 3: driver = 6, lap = 39, action = 0\n",
            "race 3: driver = 6, lap = 40, action = 0\n",
            "race 3: driver = 6, lap = 41, action = 0\n",
            "race 3: driver = 6, lap = 42, action = 0\n",
            "race 3: driver = 6, lap = 43, action = 0\n",
            "race 3: driver = 6, lap = 44, action = 2\n",
            "race 3: driver = 6, lap = 45, action = 0\n",
            "race 3: driver = 6, lap = 46, action = 0\n",
            "race 3: driver = 6, lap = 47, action = 0\n",
            "race 3: driver = 6, lap = 48, action = 0\n",
            "race 3: driver = 6, lap = 49, action = 0\n",
            "race 3: driver = 6, lap = 50, action = 0\n",
            "race 3: driver = 6, lap = 51, action = 0\n",
            "race 3: driver = 6, lap = 52, action = 0\n",
            "race 3: driver = 6, lap = 53, action = 0\n",
            "race 3: driver = 6, lap = 54, action = 0\n",
            "race 3: driver = 6, lap = 55, action = 0\n",
            "race 3: driver = 6, final_position = 18\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-8.425848180040097"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from helper_funcs.src.io import evaluate_policy\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_tf_env,\n",
        "    eval_py_env,\n",
        "    agent.policy,\n",
        "    num_episodes=3\n",
        "    # num_eval_episodes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUt0Y9fj34Qo",
        "outputId": "7d064fd4-433e-4602-ffb5-d013d0841826"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-6.033732782646257"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_policy(\n",
        "    eval_tf_env,\n",
        "    eval_py_env,\n",
        "    agent.policy,\n",
        "    num_episodes=num_eval_episodes,\n",
        "    print_lap_decisions=False\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V53cwz97fNsW"
      },
      "source": [
        "## Train the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "1. collect data from the environment\n",
        "2. use that data to train the agent's neural network\n",
        "\n",
        "Periodically, we evaluate the policy and print the cur rent score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W28I1imVfPuZ",
        "outputId": "7e321ca4-18f0-4bdc-b3c5-ef9b2c837da7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5000 [00:00<?, ?it/s]WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "step=5000, average return=-7.486: 100%|█████████▉| 4997/5000 [04:55<00:00, 24.45it/s]WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n",
            "step=5000, average return=-7.486: 100%|██████████| 5000/5000 [04:58<00:00, 16.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 55s, sys: 3.66 s, total: 5min 59s\n",
            "Wall time: 6min 35s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# reset training step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# evaluate the agent's policy once before training\n",
        "eval_tf_env.reset()\n",
        "eval_driver.run()\n",
        "rewards = [eval_avg_return_metric.result()]\n",
        "results = [evaluate_policy(\n",
        "    eval_tf_env,\n",
        "    eval_py_env,\n",
        "    agent.policy,\n",
        "    num_episodes=3\n",
        "    # num_eval_episodes\n",
        ")]\n",
        "\n",
        "# reset the environment\n",
        "time_step = train_tf_env.reset()\n",
        "\n",
        "for _ in (pbar := trange(num_iterations)):\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    time_step, policy_state = driver.run()\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, _ = next(dataset_iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "    step = int(agent.train_step_counter.numpy())\n",
        "\n",
        "    # Update progress bar status\n",
        "    if step % log_interval == 0:\n",
        "        pbar.set_description(f\"{step=}, {train_loss=:.3f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    if step % eval_interval == 0:\n",
        "        pbar.set_description(f\"Evaluating. {step=}\")\n",
        "        eval_tf_env.reset()\n",
        "        eval_driver.run()\n",
        "        rewards.append(eval_avg_return_metric.result())\n",
        "        results.append(evaluate_policy(\n",
        "            eval_tf_env,\n",
        "            eval_py_env,\n",
        "            agent.policy,\n",
        "            num_episodes=3\n",
        "            # num_eval_episodes\n",
        "        ))\n",
        "        pbar.set_description(\n",
        "            f\"{step=}, average return={rewards[-1]:.3f}\")\n",
        "\n",
        "    # Checkpoint / save models\n",
        "    if step % checkpoint_interval == 0:\n",
        "        train_checkpointer.save(global_step)\n",
        "        policy_saver.save(policy_dir)\n",
        "        save_preprocessor(train_py_env, export_dir / f\"{today}-{step}\", race=race)\n",
        "        save_policy_tflite(policy_dir, export_dir / f\"{today}-{step}\", race=race)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CxzrcjykuoLI"
      },
      "source": [
        "## Evaluate the agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K_7ujmw6vuhH"
      },
      "source": [
        "Rewards at evaluation points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHeVYCMpvsAj",
        "outputId": "a5d89f90-51d6-451b-df68-2be3c8d0009c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=-10.646892>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=-7.485831>]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rewards"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6232laOEwJep"
      },
      "source": [
        "Run a few episodes using learned agent policy. (equivalent to eval_driver metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjNa7jTnutVA",
        "outputId": "7ce1cdbd-1ec8-47e8-8498-4dfbc3a641c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "race 1: driver = 9, lap = 1, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 2, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 3, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 4, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 5, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 6, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 7, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 8, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 9, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 10, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 11, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 12, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 13, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 14, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 15, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 16, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 17, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 18, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 19, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 20, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 21, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 22, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 23, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 24, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 25, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 26, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 27, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 28, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 29, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 30, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 31, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 32, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 33, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 34, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 35, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 36, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 37, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 38, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 39, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 40, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 41, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 42, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 43, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 44, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 45, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 46, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 47, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 48, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 49, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 50, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 51, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 52, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 53, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 54, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, lap = 55, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 1: driver = 9, final_position = 18\n",
            "race 2: driver = 3, lap = 1, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 2, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 3, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 4, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 5, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 6, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 7, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 8, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 9, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 10, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 11, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 12, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 13, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 14, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 15, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 16, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 17, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 18, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 19, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 20, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 21, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 22, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 23, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 24, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 25, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 26, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 27, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 28, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 29, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 30, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 31, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 32, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 33, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 34, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 35, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 36, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 37, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 38, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 39, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 40, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 41, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 42, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 43, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 44, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 45, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 46, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 47, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 48, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 49, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 50, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 51, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 52, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 53, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 54, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, lap = 55, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 2: driver = 3, final_position = 18\n",
            "race 3: driver = 8, lap = 1, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 2, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 3, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 4, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 5, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 6, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 7, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 8, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 9, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 10, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 11, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 12, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 13, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 14, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 15, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 16, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 17, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 18, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 19, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 20, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 21, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 22, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 23, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 24, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 25, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 26, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 27, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 28, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 29, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 30, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 31, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 32, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 33, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 34, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 35, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 36, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 37, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 38, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 39, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 40, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 41, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 42, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 43, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 44, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 45, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 46, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 47, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 48, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 49, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 50, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 51, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 52, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 53, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 54, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, lap = 55, action = PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "race 3: driver = 8, final_position = 16\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-8.930442159723802"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from helper_funcs.src.io import evaluate_policy\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_tf_env,\n",
        "    eval_py_env,\n",
        "    agent.policy,\n",
        "    num_episodes=3\n",
        "    # num_eval_episodes\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb1MdIV7yWmP"
      },
      "source": [
        "More precise estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "zdeDBXVryWFW",
        "outputId": "6528005f-3c1f-4947-a0a1-0f519b91bb28"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e825f3511a58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m evaluate_policy(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0meval_tf_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0meval_py_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: evaluate_policy() got an unexpected keyword argument 'print_lap_decisions'"
          ]
        }
      ],
      "source": [
        "evaluate_policy(\n",
        "    eval_tf_env,\n",
        "    eval_py_env,\n",
        "    agent.policy,\n",
        "    num_episodes=num_eval_episodes,\n",
        "    print_lap_decisions=False,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PwukRGyc6MpQ"
      },
      "source": [
        "## Save policy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oho_64R_E7fP"
      },
      "source": [
        "Checkpoint model at the end of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXFBc-Y9tBTA",
        "outputId": "79a1ecb6-7ac5-48fa-da26-56ff98c44c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/checkpoint\n"
          ]
        }
      ],
      "source": [
        "train_checkpointer.save(global_step)\n",
        "print(f\"Saved checkpoint: {checkpoint_dir}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AFc87Xd_I-s0"
      },
      "source": [
        "Save preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1fEZ-CwHOEQ",
        "outputId": "2d5cdf9f-3891-4c29-976a-5f4d24794625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved_preprocessor=PosixPath('/content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/exports/2023-05-22-final/preprocessor_reinforcement_Shanghai_2019.pkl')\n"
          ]
        }
      ],
      "source": [
        "saved_preprocessor = save_preprocessor(train_py_env, export_dir / f\"{today}-final\", race=race)\n",
        "print(f\"{saved_preprocessor=}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne2sv_pIHKRZ"
      },
      "source": [
        "Save the policy\n",
        "\n",
        "Converts Q Network to TFlite. See [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP2zuvqAJByj",
        "outputId": "fd62d225-fe91-42ff-cbcd-3e4d8d640292"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_8_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved policy: /content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/policy\n",
            "saved_tflite=PosixPath('/content/gdrive/MyDrive/RLF002/vse-004-from-basestrat/exports/2023-05-22-final/nn_reinforcement_Shanghai_2019.tflite')\n"
          ]
        }
      ],
      "source": [
        "policy_saver.save(policy_dir)\n",
        "print(f\"Saved policy: {policy_dir}\")\n",
        "\n",
        "saved_tflite = save_policy_tflite(policy_dir, export_dir / f\"{today}-final\", race=race)\n",
        "print(f\"{saved_tflite=}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_NurPUmVt1lK"
      },
      "source": [
        "Download the checkpoint and policy zip files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WDF7hRqtsm9"
      },
      "outputs": [],
      "source": [
        "# download_archive(exported_checkpoint)\n",
        "# download_archive(exported_policy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZGN9qGxOlS"
      },
      "source": [
        "At this point, you can either (1) continue training iterations, or (2) generate an artifact to check the performance of the loaded policy, or (3) save the policy.\n",
        "\n",
        "When you save the policy and restore it, you cannot continue with the training, but you can deploy the model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8HxU1frr6RzO"
      },
      "source": [
        "## Evaluate TFLite model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XDgDq5t_y4BM"
      },
      "source": [
        "Create environment and run a few races."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTvsb-d36SrZ",
        "outputId": "a3be9c52-25be-4a40-ae3f-dd0f65343582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Evaluating reinforcement VSE by average returns and positions over 3 races against basestrategy VSE...\n",
            "race 1: driver = 3, lap = 1, action = 0\n",
            "race 1: driver = 3, lap = 2, action = 0\n",
            "race 1: driver = 3, lap = 3, action = 0\n",
            "race 1: driver = 3, lap = 4, action = 0\n",
            "race 1: driver = 3, lap = 5, action = 0\n",
            "race 1: driver = 3, lap = 6, action = 0\n",
            "race 1: driver = 3, lap = 7, action = 0\n",
            "race 1: driver = 3, lap = 8, action = 0\n",
            "race 1: driver = 3, lap = 9, action = 0\n",
            "race 1: driver = 3, lap = 10, action = 0\n",
            "race 1: driver = 3, lap = 11, action = 0\n",
            "race 1: driver = 3, lap = 12, action = 0\n",
            "race 1: driver = 3, lap = 13, action = 0\n",
            "race 1: driver = 3, lap = 14, action = 0\n",
            "race 1: driver = 3, lap = 15, action = 0\n",
            "race 1: driver = 3, lap = 16, action = 0\n",
            "race 1: driver = 3, lap = 17, action = 0\n",
            "race 1: driver = 3, lap = 18, action = 0\n",
            "race 1: driver = 3, lap = 19, action = 1\n",
            "race 1: driver = 3, lap = 20, action = 0\n",
            "race 1: driver = 3, lap = 21, action = 0\n",
            "race 1: driver = 3, lap = 22, action = 0\n",
            "race 1: driver = 3, lap = 23, action = 0\n",
            "race 1: driver = 3, lap = 24, action = 0\n",
            "race 1: driver = 3, lap = 25, action = 0\n",
            "race 1: driver = 3, lap = 26, action = 0\n",
            "race 1: driver = 3, lap = 27, action = 0\n",
            "race 1: driver = 3, lap = 28, action = 0\n",
            "race 1: driver = 3, lap = 29, action = 0\n",
            "race 1: driver = 3, lap = 30, action = 0\n",
            "race 1: driver = 3, lap = 31, action = 0\n",
            "race 1: driver = 3, lap = 32, action = 0\n",
            "race 1: driver = 3, lap = 33, action = 0\n",
            "race 1: driver = 3, lap = 34, action = 0\n",
            "race 1: driver = 3, lap = 35, action = 0\n",
            "race 1: driver = 3, lap = 36, action = 0\n",
            "race 1: driver = 3, lap = 37, action = 0\n",
            "race 1: driver = 3, lap = 38, action = 0\n",
            "race 1: driver = 3, lap = 39, action = 0\n",
            "race 1: driver = 3, lap = 40, action = 0\n",
            "race 1: driver = 3, lap = 41, action = 0\n",
            "race 1: driver = 3, lap = 42, action = 2\n",
            "race 1: driver = 3, lap = 43, action = 0\n",
            "race 1: driver = 3, lap = 44, action = 0\n",
            "race 1: driver = 3, lap = 45, action = 0\n",
            "race 1: driver = 3, lap = 46, action = 0\n",
            "race 1: driver = 3, lap = 47, action = 0\n",
            "race 1: driver = 3, lap = 48, action = 0\n",
            "race 1: driver = 3, lap = 49, action = 0\n",
            "race 1: driver = 3, lap = 50, action = 0\n",
            "race 1: driver = 3, lap = 51, action = 0\n",
            "race 1: driver = 3, lap = 52, action = 0\n",
            "race 1: driver = 3, lap = 53, action = 0\n",
            "race 1: driver = 3, lap = 54, action = 0\n",
            "race 1: driver = 3, lap = 55, action = 0\n",
            "race 1: driver = 3, final_position = 16\n",
            "INFO: Progress: |████████████████----------------------------------| 33.3% race 2: driver = 13, lap = 1, action = 0\n",
            "race 2: driver = 13, lap = 2, action = 0\n",
            "race 2: driver = 13, lap = 3, action = 0\n",
            "race 2: driver = 13, lap = 4, action = 0\n",
            "race 2: driver = 13, lap = 5, action = 0\n",
            "race 2: driver = 13, lap = 6, action = 0\n",
            "race 2: driver = 13, lap = 7, action = 0\n",
            "race 2: driver = 13, lap = 8, action = 0\n",
            "race 2: driver = 13, lap = 9, action = 0\n",
            "race 2: driver = 13, lap = 10, action = 0\n",
            "race 2: driver = 13, lap = 11, action = 0\n",
            "race 2: driver = 13, lap = 12, action = 0\n",
            "race 2: driver = 13, lap = 13, action = 0\n",
            "race 2: driver = 13, lap = 14, action = 0\n",
            "race 2: driver = 13, lap = 15, action = 0\n",
            "race 2: driver = 13, lap = 16, action = 0\n",
            "race 2: driver = 13, lap = 17, action = 0\n",
            "race 2: driver = 13, lap = 18, action = 0\n",
            "race 2: driver = 13, lap = 19, action = 0\n",
            "race 2: driver = 13, lap = 20, action = 2\n",
            "race 2: driver = 13, lap = 21, action = 0\n",
            "race 2: driver = 13, lap = 22, action = 0\n",
            "race 2: driver = 13, lap = 23, action = 0\n",
            "race 2: driver = 13, lap = 24, action = 0\n",
            "race 2: driver = 13, lap = 25, action = 0\n",
            "race 2: driver = 13, lap = 26, action = 0\n",
            "race 2: driver = 13, lap = 27, action = 0\n",
            "race 2: driver = 13, lap = 28, action = 0\n",
            "race 2: driver = 13, lap = 29, action = 0\n",
            "race 2: driver = 13, lap = 30, action = 0\n",
            "race 2: driver = 13, lap = 31, action = 0\n",
            "race 2: driver = 13, lap = 32, action = 0\n",
            "race 2: driver = 13, lap = 33, action = 0\n",
            "race 2: driver = 13, lap = 34, action = 0\n",
            "race 2: driver = 13, lap = 35, action = 0\n",
            "race 2: driver = 13, lap = 36, action = 0\n",
            "race 2: driver = 13, lap = 37, action = 0\n",
            "race 2: driver = 13, lap = 38, action = 0\n",
            "race 2: driver = 13, lap = 39, action = 0\n",
            "race 2: driver = 13, lap = 40, action = 0\n",
            "race 2: driver = 13, lap = 41, action = 0\n",
            "race 2: driver = 13, lap = 42, action = 0\n",
            "race 2: driver = 13, lap = 43, action = 0\n",
            "race 2: driver = 13, lap = 44, action = 0\n",
            "race 2: driver = 13, lap = 45, action = 0\n",
            "race 2: driver = 13, lap = 46, action = 0\n",
            "race 2: driver = 13, lap = 47, action = 0\n",
            "race 2: driver = 13, lap = 48, action = 0\n",
            "race 2: driver = 13, lap = 49, action = 0\n",
            "race 2: driver = 13, lap = 50, action = 0\n",
            "race 2: driver = 13, lap = 51, action = 0\n",
            "race 2: driver = 13, lap = 52, action = 0\n",
            "race 2: driver = 13, lap = 53, action = 0\n",
            "race 2: driver = 13, lap = 54, action = 0\n",
            "race 2: driver = 13, lap = 55, action = 0\n",
            "race 2: driver = 13, final_position = 3\n",
            "INFO: Progress: |█████████████████████████████████-----------------| 66.7% race 3: driver = 5, lap = 1, action = 2\n",
            "race 3: driver = 5, lap = 2, action = 0\n",
            "race 3: driver = 5, lap = 3, action = 0\n",
            "race 3: driver = 5, lap = 4, action = 0\n",
            "race 3: driver = 5, lap = 5, action = 0\n",
            "race 3: driver = 5, lap = 6, action = 0\n",
            "race 3: driver = 5, lap = 7, action = 0\n",
            "race 3: driver = 5, lap = 8, action = 0\n",
            "race 3: driver = 5, lap = 9, action = 0\n",
            "race 3: driver = 5, lap = 10, action = 0\n",
            "race 3: driver = 5, lap = 11, action = 0\n",
            "race 3: driver = 5, lap = 12, action = 0\n",
            "race 3: driver = 5, lap = 13, action = 0\n",
            "race 3: driver = 5, lap = 14, action = 2\n",
            "race 3: driver = 5, lap = 15, action = 0\n",
            "race 3: driver = 5, lap = 16, action = 0\n",
            "race 3: driver = 5, lap = 17, action = 0\n",
            "race 3: driver = 5, lap = 18, action = 0\n",
            "race 3: driver = 5, lap = 19, action = 0\n",
            "race 3: driver = 5, lap = 20, action = 0\n",
            "race 3: driver = 5, lap = 21, action = 0\n",
            "race 3: driver = 5, lap = 22, action = 0\n",
            "race 3: driver = 5, lap = 23, action = 0\n",
            "race 3: driver = 5, lap = 24, action = 0\n",
            "race 3: driver = 5, lap = 25, action = 0\n",
            "race 3: driver = 5, lap = 26, action = 0\n",
            "race 3: driver = 5, lap = 27, action = 0\n",
            "race 3: driver = 5, lap = 28, action = 0\n",
            "race 3: driver = 5, lap = 29, action = 0\n",
            "race 3: driver = 5, lap = 30, action = 2\n",
            "race 3: driver = 5, lap = 31, action = 0\n",
            "race 3: driver = 5, lap = 32, action = 0\n",
            "race 3: driver = 5, lap = 33, action = 0\n",
            "race 3: driver = 5, lap = 34, action = 0\n",
            "race 3: driver = 5, lap = 35, action = 0\n",
            "race 3: driver = 5, lap = 36, action = 0\n",
            "race 3: driver = 5, lap = 37, action = 0\n",
            "race 3: driver = 5, lap = 38, action = 0\n",
            "race 3: driver = 5, lap = 39, action = 0\n",
            "race 3: driver = 5, lap = 40, action = 0\n",
            "race 3: driver = 5, lap = 41, action = 0\n",
            "race 3: driver = 5, lap = 42, action = 0\n",
            "race 3: driver = 5, lap = 43, action = 0\n",
            "race 3: driver = 5, lap = 44, action = 0\n",
            "race 3: driver = 5, lap = 45, action = 0\n",
            "race 3: driver = 5, lap = 46, action = 0\n",
            "race 3: driver = 5, lap = 47, action = 0\n",
            "race 3: driver = 5, lap = 48, action = 0\n",
            "race 3: driver = 5, lap = 49, action = 0\n",
            "race 3: driver = 5, lap = 50, action = 0\n",
            "race 3: driver = 5, lap = 51, action = 0\n",
            "race 3: driver = 5, lap = 52, action = 0\n",
            "race 3: driver = 5, lap = 53, action = 0\n",
            "race 3: driver = 5, lap = 54, action = 0\n",
            "race 3: driver = 5, lap = 55, action = 0\n",
            "race 3: driver = 5, final_position = 11\n",
            "INFO: Progress: |██████████████████████████████████████████████████| 100.0% \n",
            "RESULT: Average return (total): -10.281 (FCY: -10.006, no FCY: -10.830), average position (total): 10.0 (FCY: 13.5, no FCY: 3.0), FCY races: 2, no FCY races: 1\n"
          ]
        }
      ],
      "source": [
        "from machine_learning_rl_training.src.rl_evaluate_policy import print_returns_positions\n",
        "\n",
        "py_env = SingleAgentRaceSimulation(\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file,\n",
        "    vse_type=vse_others,\n",
        "    use_prob_infl=True,\n",
        "    create_rand_events=True,\n",
        "    vse_paths=vse_paths)\n",
        "\n",
        "\n",
        "print_returns_positions(\n",
        "    py_env=py_env,\n",
        "    num_races=3,\n",
        "    tf_lite_path=str(saved_tflite),\n",
        "    vse_others=vse_others_postproc,\n",
        "    print_lap_decisions=True,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7sPMapiwy8DD"
      },
      "source": [
        "Get a more precise estimate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6ohBJs65CmH",
        "outputId": "8575945c-be8c-4103-885b-3c5dc83d807b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_races_postproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyKqT8Iuy1Eh",
        "outputId": "420c37fc-e0b7-4f0a-ac45-9af2ac8010b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Evaluating reinforcement VSE by average returns and positions over 100 races against basestrategy VSE...\n",
            "INFO: Progress: |██████████████████████████████████████████████████| 100.0% \n",
            "RESULT: Average return (total): -4.210 (FCY: -3.736, no FCY: -5.560), average position (total): 9.6 (FCY: 9.2, no FCY: 10.8), FCY races: 74, no FCY races: 26\n"
          ]
        }
      ],
      "source": [
        "print_returns_positions(\n",
        "    py_env=py_env,\n",
        "    num_races=num_races_postproc,\n",
        "    tf_lite_path=str(saved_tflite),\n",
        "    vse_others=vse_others_postproc,\n",
        "    print_lap_decisions=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7br4f5DxCdx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
