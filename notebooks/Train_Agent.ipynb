{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "isD2FGz19Q1w"
      },
      "source": [
        "# Training the agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9wugzXUvEKuO"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "foVr8R3wD76l"
      },
      "outputs": [],
      "source": [
        "# workspace directory \n",
        "WORKSPACE_DIR = \"/content/gdrive/MyDrive/RLF002/vse-002\"\n",
        "\n",
        "# environment parameters\n",
        "race = \"Shanghai_2019\"  # set race (see racesim/input/parameters for possible races)\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available),\n",
        "# 'multi_agent' (if VSE should learn for all drivers at once)\n",
        "vse_others = \"basestrategy\"\n",
        "mcs_pars_file = \"pars_mcs.ini\"  # parameter file for Monte Carlo parameters\n",
        "race_pars_file = f\"pars_{race}.ini\"\n",
        "\n",
        "# hyperparameters\n",
        "num_iterations = 1\n",
        "replay_buffer_max_length = 200_000\n",
        "initial_collect_steps = 200\n",
        "collect_steps_per_iteration = 1\n",
        "\n",
        "fc_layer_params = (64, 64,)\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "gamma = 1.0  # discount rate\n",
        "n_step_update = 1\n",
        "target_update_period = 1\n",
        "dueling_q_net = False\n",
        "\n",
        "# training options\n",
        "num_iterations = 10_000 # 100_000\n",
        "log_interval = 10_000 # 100_000\n",
        "eval_interval = 5_000 # 50_000\n",
        "checkpoint_interval = 5_000\n",
        "num_eval_episodes = 100\n",
        "\n",
        "# postprocessing (currently not implemented for multi-agent environment)\n",
        "calculate_final_positions = False  # activate or deactivate evaluation after training\n",
        "num_races_postproc = 10_000\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available)\n",
        "vse_others_postproc = \"basestrategy\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xsAPFt7LCAsp"
      },
      "source": [
        "Mount Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3_vL5uoB-oy",
        "outputId": "1a1e4ebf-a1b3-483f-9315-decc6ddaad17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T3goytPRB8Z7"
      },
      "source": [
        "Check Colab settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3ioujpmB3fX",
        "outputId": "fa660d14-44c5-4892-df2f-6304bf590851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = \"\\n\".join(gpu_info)\n",
        "if gpu_info.find(\"failed\") >= 0:\n",
        "  print(\"Not connected to a GPU\")\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n\")\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print(\"Not using a high-RAM runtime\")\n",
        "else:\n",
        "  print(\"You are using a high-RAM runtime!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xKLl8tLs9eLt"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7DcaHnmrCH3A"
      },
      "source": [
        "Install code repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKv7Rl2kCDZJ",
        "outputId": "cf741cf8-0973-4019-c7b1-9acc7a7e132e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'msca-race-simulation'...\n",
            "remote: Enumerating objects: 363, done.\u001b[K\n",
            "remote: Counting objects: 100% (363/363), done.\u001b[K\n",
            "remote: Compressing objects: 100% (302/302), done.\u001b[K\n",
            "remote: Total 363 (delta 177), reused 214 (delta 52), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (363/363), 3.65 MiB | 8.88 MiB/s, done.\n",
            "Resolving deltas: 100% (177/177), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/pezon/msca-race-simulation \n",
        "!cp -R msca-race-simulation/* ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N622WipICSdQ"
      },
      "source": [
        "Install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC95MIfJCQQR",
        "outputId": "bb42f01b-dd82-4013-f9d9-3d739cce5b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.20.1)\n",
            "Collecting tf-agents (from -r requirements.txt (line 15))\n",
            "  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (3.2.3)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (0.1.8)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents->-r requirements.txt (line 15))\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from tf-agents->-r requirements.txt (line 15))\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability (from -r requirements.txt (line 14))\n",
            "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 11)) (0.40.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents->-r requirements.txt (line 15)) (0.0.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 11)) (0.1.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy->-r requirements.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697641 sha256=bc625efc18c05c4a06f9dc5845b0960f9fceccff7cf6e5359ab52b27770de759\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf-agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.20.1\n",
            "    Uninstalling tensorflow-probability-0.20.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.20.1\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.19.0 tf-agents-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OXzDj4Qo9lzT"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_EdZK9z_Ckap"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3e4_IcMbCjqN"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.metrics.tf_metrics import AverageReturnMetric\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.policies.py_tf_eager_policy import PyTFEagerPolicy\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer \\\n",
        "  import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tqdm import trange\n",
        "\n",
        "from helper_funcs.src.io import save_preprocessor, save_policy_tflite\n",
        "from machine_learning_rl_training.src.rl_environment_multi_agent \\\n",
        "  import RaceSimulation as MultiAgentRaceSimulation\n",
        "from machine_learning_rl_training.src.rl_environment_single_agent \\\n",
        "  import RaceSimulation as SingleAgentRaceSimulation\n",
        "from racesim.src.import_pars import import_pars\n",
        " \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set working directory\n",
        "workspace_dir = Path(WORKSPACE_DIR)\n",
        "checkpoint_dir = workspace_dir / \"checkpoint\"\n",
        "policy_dir = workspace_dir / \"policy\"\n",
        "export_dir = workspace_dir / \"exports\"\n",
        "\n",
        "# Create directories\n",
        "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
        "policy_dir.mkdir(exist_ok=True, parents=True)\n",
        "export_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m1gJsHkBCf2L"
      },
      "source": [
        "Check training input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tcKh0ricCUHn"
      },
      "outputs": [],
      "source": [
        "if vse_others == \"multi_agent\" and calculate_final_positions:\n",
        "    print(\"WARNING: Evaluation of trained strategy is currently not implemented for the multi-agent environment!\"\n",
        "          \" Setting calculate_final_positions = False!\")\n",
        "    calculate_final_positions = False\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "# CHECK FOR WET RACE ---------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# load parameter file\n",
        "pars_in = import_pars(\n",
        "    use_print=False,\n",
        "    use_vse=False,\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file)[0]\n",
        "\n",
        "# loop through drivers and check for intermediate or wet tire compounds in real race\n",
        "for driver in pars_in[\"driver_pars\"]:\n",
        "    if any([True if strat[1] in [\"I\", \"W\"] else False for strat in pars_in[\"driver_pars\"][driver][\"strategy_info\"]]):\n",
        "        raise RuntimeError(f\"Cannot train for current race {race} because it was a (partly) wet race!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KEEifSb-C9GA"
      },
      "source": [
        "### Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkvmZh8CvHa",
        "outputId": "0f689ac6-3179-4fdc-c284-21b353562e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Race: Shanghai_2019, strategy of other drivers: basestrategy\n",
            "INFO: Observation spec: BoundedArraySpec(shape=(40,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n",
            "INFO: Action spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3)\n"
          ]
        }
      ],
      "source": [
        "if vse_others == \"multi_agent\":\n",
        "    train_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "    eval_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "else:\n",
        "    train_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "    eval_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "\n",
        "train_tf_env = TFPyEnvironment(environment=train_py_env)\n",
        "eval_tf_env = TFPyEnvironment(environment=eval_py_env)\n",
        "\n",
        "print(f\"INFO: Race: {race}, strategy of other drivers: {vse_others}\")\n",
        "if train_py_env.batched:\n",
        "    print(f\"INFO: Batched environment: {train_py_env.batched()}, batch size: {train_py_env.batch_size}\")\n",
        "print(f\"INFO: Observation spec: {train_py_env.time_step_spec().observation}\")\n",
        "print(f\"INFO: Action spec: {train_py_env.action_spec()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UYYbF0q1DkQy"
      },
      "source": [
        "### Setup DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J846kRl4Dbuh"
      },
      "outputs": [],
      "source": [
        "q_net = QNetwork(\n",
        "    input_tensor_spec=train_tf_env.observation_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "boltzmann_fn = PolynomialDecay(\n",
        "    initial_learning_rate=1.0,\n",
        "    decay_steps=num_iterations,\n",
        "    end_learning_rate=0.01)\n",
        "\n",
        "agent = DqnAgent(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    n_step_update=n_step_update,\n",
        "    target_update_period=target_update_period,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=global_step)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "93rbkpEHEU4g"
      },
      "source": [
        "### Setup policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QwgMb9TeEMJ-"
      },
      "outputs": [],
      "source": [
        "random_policy = RandomTFPolicy(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec())\n",
        "\n",
        "eager_policy = PyTFEagerPolicy(\n",
        "    agent.collect_policy,\n",
        "    use_tf_function=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NdD8lAxHElqd"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We use a Driver to collect experience in an environment. To use a Driver, we specify an observer `replay_buffer.add_batch` that instructs the driver to add trajectory elements to the replay buffer when it receives a trajectory. \n",
        "\n",
        "Then we run the experience collecting loop using the driver.\n",
        "\n",
        "Source: [DynamicStepDriver | TensorFlow Documentation](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCwIZeCWElEe",
        "outputId": "a91996d0-25c3-4ff3-f966-91c7d9ba0969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 40), dtype=float32, numpy=\n",
            "array([[0.64285713, 1.        , 0.        , 0.01785714, 0.        ,\n",
            "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
            "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.3947687], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}) ()\n",
            "-120.35165\n"
          ]
        }
      ],
      "source": [
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_tf_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "driver = DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[\n",
        "        replay_buffer.add_batch,\n",
        "        avg_return_metric,\n",
        "    ],\n",
        "    num_steps=collect_steps_per_iteration)\n",
        "\n",
        "# Initial data collection:\n",
        "# initial driver.run will reset the environment and initialize the policy\n",
        "for _ in range(initial_collect_steps):\n",
        "    final_time_step, policy_state = driver.run()\n",
        "print(final_time_step, policy_state)\n",
        "print(avg_return_metric.result().numpy())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZziMiUDKHSM"
      },
      "source": [
        "Reading data for a train step\n",
        "\n",
        "After adding trajectory elements to the replay buffer, we can read batches of trajectory fom the replay buffer to use as input for a train step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWXBSqxrKNv9",
        "outputId": "725dd607-7bf0-4bb5-f86a-43af76e129b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.counter(...)` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "# Dataset generates trajectories with shape [BxTx...] where\n",
        "# T = n_step_update + 1.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2\n",
        ").prefetch(3)\n",
        "\n",
        "# inspection:\n",
        "dataset_iterator = iter(dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CVRHU6bWB7ba"
      },
      "source": [
        "### Setup checkpointing and saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kiT1pZ5iB7uM"
      },
      "outputs": [],
      "source": [
        "from tf_agents.policies.policy_saver import PolicySaver\n",
        "from tf_agents.utils.common import Checkpointer\n",
        "\n",
        "train_checkpointer = Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=20,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")\n",
        "\n",
        "policy_saver = PolicySaver(agent.policy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AKLCaym3Dntf"
      },
      "source": [
        "If there is checkpoint saved in the working directory, it will be restored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p4Facv1DYz3",
        "outputId": "a601b962-a3f9-42b5-aa20-a25059086323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restoring checkpoint: /content/gdrive/MyDrive/RLF002/vse-002/checkpoint\n"
          ]
        }
      ],
      "source": [
        "print(f\"Restoring checkpoint: {checkpoint_dir}\")\n",
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NzpeKPNjhsBD"
      },
      "source": [
        "### Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "76H0rq3Qhtz2"
      },
      "outputs": [],
      "source": [
        "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
        "\n",
        "eval_avg_return_metric = AverageReturnMetric()\n",
        "\n",
        "eval_driver = DynamicEpisodeDriver(\n",
        "    eval_tf_env,\n",
        "    agent.policy,\n",
        "    observers=[\n",
        "        eval_avg_return_metric,\n",
        "    ],\n",
        "    num_episodes=num_eval_episodes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V53cwz97fNsW"
      },
      "source": [
        "### Train the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "1. collect data from the environment\n",
        "2. use that data to train the agent's neural network\n",
        "\n",
        "Periodically, we evaluate the policy and print the cur rent score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W28I1imVfPuZ",
        "outputId": "556632d0-26ba-4b56-ec8f-fb07487017a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "  4%|▍         | 408/10000 [00:15<04:35, 34.77it/s]"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# reset training step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# evaluate the agent's policy once before training\n",
        "eval_tf_env.reset()\n",
        "eval_driver.run()\n",
        "rewards = [eval_avg_return_metric.result()]\n",
        "\n",
        "# reset the environment\n",
        "time_step = train_tf_env.reset()\n",
        "\n",
        "for _ in (pbar := trange(num_iterations)):\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    time_step, policy_state = driver.run()\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, _ = next(dataset_iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "    step = int(agent.train_step_counter.numpy())\n",
        "\n",
        "    # Update progress bar status\n",
        "    if step % log_interval == 0:\n",
        "        pbar.set_description(f\"{step=}, {train_loss=:.3f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    if step % eval_interval == 0:\n",
        "        pbar.set_description(f\"Evaluating. {step=}\")\n",
        "        eval_tf_env.reset()\n",
        "        eval_driver.run()\n",
        "        rewards.append(eval_avg_return_metric.result())\n",
        "        pbar.set_description(\n",
        "            f\"{step=}, average return={rewards[-1]:.3f}\")\n",
        "\n",
        "    # Checkpoint / save models\n",
        "    if step % checkpoint_interval == 0:\n",
        "        train_checkpointer.save(global_step)\n",
        "        policy_saver.save(policy_dir)\n",
        "        save_preprocessor(train_py_env, export_dir / f\"{today}-{step}\", race=race)\n",
        "        save_policy_tflite(policy_dir, export_dir / f\"{today}-{step}\", race=race)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PwukRGyc6MpQ"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oho_64R_E7fP"
      },
      "source": [
        "Checkpoint model at the end of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXFBc-Y9tBTA"
      },
      "outputs": [],
      "source": [
        "train_checkpointer.save(global_step)\n",
        "print(f\"Saved checkpoint: {checkpoint_dir}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AFc87Xd_I-s0"
      },
      "source": [
        "Save preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1fEZ-CwHOEQ"
      },
      "outputs": [],
      "source": [
        "saved_preprocessor = save_preprocessor(train_py_env, export_dir / f\"{today}-final\", race=race)\n",
        "print(f\"{saved_preprocessor=}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne2sv_pIHKRZ"
      },
      "source": [
        "Save the policy\n",
        "\n",
        "Converts Q Network to TFlite. See [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP2zuvqAJByj"
      },
      "outputs": [],
      "source": [
        "policy_saver.save(policy_dir)\n",
        "print(f\"Saved policy: {policy_dir}\")\n",
        "\n",
        "saved_tflite = save_policy_tflite(policy_dir, export_dir / f\"{today}-{step}\", race=race)\n",
        "print(f\"{saved_tflite=}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_NurPUmVt1lK"
      },
      "source": [
        "Download the checkpoint and policy zip files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WDF7hRqtsm9"
      },
      "outputs": [],
      "source": [
        "# download_archive(exported_checkpoint)\n",
        "# download_archive(exported_policy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZGN9qGxOlS"
      },
      "source": [
        "At this point, you can either (1) continue training iterations, or (2) generate an artifact to check the performance of the loaded policy, or (3) save the policy.\n",
        "\n",
        "When you save the policy and restore it, you cannot continue with the training, but you can deploy the model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8HxU1frr6RzO"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "yTvsb-d36SrZ",
        "outputId": "db6df6cd-7ffd-4586-d560-4c2990ceaddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Evaluating reinforcement VSE by average returns and positions over 10000 races against basestrategy VSE...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8c4a177e5857>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m print_returns_positions(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mpy_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpy_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnum_races\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_races_postproc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/machine_learning_rl_training/src/rl_evaluate_policy.py\u001b[0m in \u001b[0;36mprint_returns_positions\u001b[0;34m(py_env, num_races, tf_lite_path, vse_others)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# set NN input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mq_net_lite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interpreter\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_net_lite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_time_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# invoke NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mset_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mresize_tensor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type FLOAT32 but expected type INT32 for input 0, name: action_0_step_type:0 "
          ]
        }
      ],
      "source": [
        "from machine_learning_rl_training.src.rl_evaluate_policy import print_returns_positions\n",
        "\n",
        "\n",
        "py_env = SingleAgentRaceSimulation(\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file,\n",
        "    vse_type=vse_others,\n",
        "    use_prob_infl=True,\n",
        "    create_rand_events=True)\n",
        "\n",
        "\n",
        "print_returns_positions(\n",
        "    py_env=py_env,\n",
        "    num_races=num_races_postproc,\n",
        "    tf_lite_path=str(saved_tflite),\n",
        "    vse_others=vse_others_postproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COYUhx0L7w2M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
