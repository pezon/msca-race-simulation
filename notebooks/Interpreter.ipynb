{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLite Interpreter\n",
    "\n",
    "This code is based on `rl_evaluate_policy` package `print_returns_positions module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "\n",
    "from machine_learning_rl_training.src.rl_environment_single_agent import RaceSimulation\n",
    "from machine_learning_rl_training.src.rl_evaluate_policy import convert_time_step, print_returns_positions\n",
    "\n",
    "model_path = \"/content/gdrive/MyDrive/RLF002/vse-002/exports/2023-05-21-final/nn_reinforcement_Shanghai_2019.tflite\"\n",
    "race_pars_file = \"/content/racesim/input/parameters/pars_Shanghai_2019.ini\"\n",
    "mcs_pars_file = \"/content/racesim/input/parameters/pars_mcs.ini\"\n",
    "vse_others = \"basestrategy\"\n",
    "\n",
    "py_env = RaceSimulation(\n",
    "    race_pars_file=race_pars_file,\n",
    "    mcs_pars_file=mcs_pars_file,\n",
    "    vse_type=vse_others,\n",
    "    use_prob_infl=True,\n",
    "    create_rand_events=True)\n",
    "\n",
    "print(\n",
    "  py_env.action_spec(),\n",
    "  py_env.observation_spec(),\n",
    ")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "# input_index = interpreter.get_input_details()[0][\"index\"]  # old model\n",
    "input_index = interpreter.get_input_details()[2][\"index\"]  # new model\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = py_env.reset()\n",
    "episode_return = 0.0\n",
    "\n",
    "lap = 1\n",
    "while not time_step.is_last():\n",
    "    converted_time_step = convert_time_step(time_step)\n",
    "    observation = converted_time_step.observation\n",
    "    interpreter.set_tensor(2, converted_time_step.observation)\n",
    "    interpreter.invoke()\n",
    "    print(interpreter.get_tensor(output_index))\n",
    "    # action_q = interpreter.get_tensor(output_index)[0].argmax()  # old model\n",
    "    action_q = interpreter.get_tensor(output_index)[0]  # new model\n",
    "    print(f\"driver = {py_env.idx_driver}, lap = {lap}, action = {action_q}\")\n",
    "    time_step = py_env.step(action_q)\n",
    "    episode_return += time_step.reward\n",
    "    lap += 1\n",
    "\n",
    "final_position = py_env.race.positions[py_env.race.get_last_compl_lap(py_env.idx_driver), py_env.idx_driver]\n",
    "print(f\"driver = {py_env.idx_driver}, final_position = {final_position}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
