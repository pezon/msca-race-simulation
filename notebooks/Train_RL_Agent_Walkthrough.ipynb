{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Check Colab settings."
      ],
      "metadata": {
        "id": "T3goytPRB8Z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3ioujpmB3fX",
        "outputId": "33fcb6e7-2d24-4f28-d544-053fb94cb4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive."
      ],
      "metadata": {
        "id": "xsAPFt7LCAsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3_vL5uoB-oy",
        "outputId": "d2394055-0f90-455e-9781-4dfee5cf6263"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install code repository"
      ],
      "metadata": {
        "id": "7DcaHnmrCH3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/pezon/msca-race-simulation \n",
        "!cp -R msca-race-simulation/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKv7Rl2kCDZJ",
        "outputId": "cb960ee4-2db4-4da8-e090-f36b0dc07a32"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'msca-race-simulation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies."
      ],
      "metadata": {
        "id": "N622WipICSdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC95MIfJCQQR",
        "outputId": "fcf22695-f835-47b0-dd46-45f3d93ebe67"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.19.0)\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (3.2.3)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 11)) (0.32.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->-r requirements.txt (line 14)) (0.1.8)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (0.23.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents->-r requirements.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 11)) (0.40.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents->-r requirements.txt (line 15)) (0.0.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 11)) (0.1.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy->-r requirements.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 11)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "_EdZK9z_Ckap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  files = None\n",
        "\n",
        "from racesim.src.import_pars import import_pars\n",
        "\n",
        "\n",
        "# Set working director\n",
        "workspace_dir = Path(\"/content/gdrive/MyDrive/RLF1/vse-1\")\n",
        "\n",
        "# Set up output paths \n",
        "checkpoint_dir = os.path.join(workspace_dir, \"checkpoint\")\n",
        "policy_dir = os.path.join(workspace_dir, \"policy\")\n",
        "\n",
        "checkpoint_export_dir = workspace_dir / \"exports/checkpoints\"\n",
        "checkpoint_export_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "policy_export_dir = workspace_dir / \"exports/policy-tflite\"\n",
        "policy_export_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "policy_tflite_export_dir = workspace_dir / \"exports/policy-tflite\"\n",
        "policy_tflite_export_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "def export_archive(dirname, base_filename):\n",
        "  return shutil.make_archive(base_filename, \"zip\", dirname)\n",
        "\n",
        "\n",
        "def import_archive(dirname):\n",
        "  if files is None:\n",
        "    return\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "    shutil.rmtree(dirname)\n",
        "    zip_files = ZipFile(BytesIO(uploaded[fn]), 'r')\n",
        "    zip_files.extractall(dirname)\n",
        "    zip_files.close()"
      ],
      "metadata": {
        "id": "3e4_IcMbCjqN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure training"
      ],
      "metadata": {
        "id": "m1gJsHkBCf2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# environment parameters\n",
        "race = \"Shanghai_2019\"  # set race (see racesim/input/parameters for possible races)\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available),\n",
        "# 'multi_agent' (if VSE should learn for all drivers at once)\n",
        "vse_others = \"basestrategy\"\n",
        "mcs_pars_file = \"pars_mcs.ini\"  # parameter file for Monte Carlo parameters\n",
        "\n",
        "# hyperparameters\n",
        "num_iterations = 1\n",
        "replay_buffer_max_length = 200_000\n",
        "initial_collect_steps = 200\n",
        "collect_steps_per_iteration = 1\n",
        "\n",
        "fc_layer_params = (64, 64,)\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "gamma = 1.0  # discount rate\n",
        "n_step_update = 1\n",
        "target_update_period = 1\n",
        "dueling_q_net = False\n",
        "\n",
        "# training options\n",
        "log_interval = 100_000\n",
        "eval_interval = 50_000\n",
        "num_eval_episodes = 100\n",
        "\n",
        "# postprocessing (currently not implemented for multi-agent environment)\n",
        "calculate_final_positions = False  # activate or deactivate evaluation after training\n",
        "num_races_postproc = 10_000\n",
        "# VSE type for other drivers: 'basestrategy', 'realstrategy', 'supervised', 'reinforcement' (if already available)\n",
        "vse_others_postproc = \"basestrategy\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "# CHECK USER INPUT -----------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "if vse_others == \"multi_agent\" and calculate_final_positions:\n",
        "    print(\"WARNING: Evaluation of trained strategy is currently not implemented for the multi-agent environment!\"\n",
        "          \" Setting calculate_final_positions = False!\")\n",
        "    calculate_final_positions = False\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "# CHECK FOR WET RACE ---------------------------------------------------------------------------------------------------\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# create race parameter file name\n",
        "race_pars_file = 'pars_%s.ini' % race\n",
        "\n",
        "# load parameter file\n",
        "pars_in = import_pars(\n",
        "    use_print=False,\n",
        "    use_vse=False,\n",
        "    race_pars_file=race_pars_file,\n",
        "    mcs_pars_file=mcs_pars_file)[0]\n",
        "\n",
        "# loop through drivers and check for intermediate or wet tire compounds in real race\n",
        "for driver in pars_in[\"driver_pars\"]:\n",
        "    if any([True if strat[1] in [\"I\", \"W\"] else False for strat in pars_in[\"driver_pars\"][driver][\"strategy_info\"]]):\n",
        "        raise RuntimeError(\"Cannot train for current race %s because it was a (partly) wet race!\" % race)"
      ],
      "metadata": {
        "id": "tcKh0ricCUHn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup environment"
      ],
      "metadata": {
        "id": "KEEifSb-C9GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "\n",
        "from machine_learning_rl_training.src.rl_environment_multi_agent import RaceSimulation as MultiAgentRaceSimulation\n",
        "from machine_learning_rl_training.src.rl_environment_single_agent import RaceSimulation as SingleAgentRaceSimulation\n",
        "        \n",
        "if vse_others == 'multi_agent':\n",
        "    train_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "    eval_py_env = MultiAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "else:\n",
        "    train_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "    eval_py_env = SingleAgentRaceSimulation(\n",
        "        race_pars_file=race_pars_file,\n",
        "        mcs_pars_file=mcs_pars_file,\n",
        "        vse_type=vse_others,\n",
        "        use_prob_infl=True,\n",
        "        create_rand_events=True)\n",
        "\n",
        "train_tf_env = TFPyEnvironment(environment=train_py_env)\n",
        "eval_tf_env = TFPyEnvironment(environment=eval_py_env)\n",
        "\n",
        "print('INFO: Race: %s, strategy of other drivers: %s' % (race, vse_others))\n",
        "\n",
        "if train_py_env.batched:\n",
        "    print('INFO: Batched environment:', train_py_env.batched(), 'batch size:', train_py_env.batch_size)\n",
        "\n",
        "print('INFO: Observation spec:', train_py_env.time_step_spec().observation)\n",
        "print('INFO: Action spec:', train_py_env.action_spec())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkvmZh8CvHa",
        "outputId": "5b6e64bf-68fc-4272-865b-8f1e64c09a59"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Race: Shanghai_2019, strategy of other drivers: basestrategy\n",
            "INFO: Observation spec: BoundedArraySpec(shape=(40,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n",
            "INFO: Action spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup DQN Agent"
      ],
      "metadata": {
        "id": "UYYbF0q1DkQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.compat.v1.train import AdamOptimizer\n",
        "from tensorflow.compat.v1.train import get_or_create_global_step\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "\n",
        "q_net = QNetwork(\n",
        "    input_tensor_spec=train_tf_env.observation_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
        "global_step = get_or_create_global_step()\n",
        "\n",
        "boltzmann_fn = PolynomialDecay(\n",
        "    initial_learning_rate=1.0,\n",
        "    decay_steps=num_iterations,\n",
        "    end_learning_rate=0.01)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    n_step_update=n_step_update,\n",
        "    target_update_period=target_update_period,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=gamma,\n",
        "    train_step_counter=global_step)\n",
        "\n",
        "agent.initialize()\n"
      ],
      "metadata": {
        "id": "J846kRl4Dbuh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup policy."
      ],
      "metadata": {
        "id": "93rbkpEHEU4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "\n",
        "policy = RandomTFPolicy(\n",
        "    time_step_spec=train_tf_env.time_step_spec(),\n",
        "    action_spec=train_tf_env.action_spec())\n",
        "\n",
        "#@TODO: do we need to inject this into data collection?"
      ],
      "metadata": {
        "id": "QwgMb9TeEMJ-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection\n",
        "\n",
        "Source: [DynamicStepDriver | TensorFlow Documentation](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver)"
      ],
      "metadata": {
        "id": "NdD8lAxHElqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.metrics.tf_metrics import AverageReturnMetric, EnvironmentSteps, NumberOfEpisodes\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
        "\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_tf_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "metric = AverageReturnMetric()\n",
        "\n",
        "driver = DynamicStepDriver(\n",
        "    train_tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[\n",
        "        replay_buffer.add_batch,\n",
        "        metric,\n",
        "    ],\n",
        "    num_steps=collect_steps_per_iteration)\n",
        "\n",
        "# Initial data collection:\n",
        "# initial driver.run will reset the environment and initialize the policy\n",
        "final_time_step, policy_state = driver.run(maximum_iterations=initial_collect_steps)\n",
        "\n",
        "# Dataset generates trajectories with shape [BxTx...] where\n",
        "# T = n_step_update + 1.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2\n",
        ").prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "metadata": {
        "id": "XCwIZeCWElEe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate agent policy"
      ],
      "metadata": {
        "id": "I7GvfrUilrj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average_return(\n",
        "    env: TFPyEnvironment,\n",
        "    policy,\n",
        "    num_episodes: int = 1\n",
        ") -> float:\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "        time_step_ = env.reset()\n",
        "        episode_return = 0.0\n",
        "        while not any(time_step_.is_last()):\n",
        "            action_step = policy.action(time_step_)\n",
        "            time_step_ = env.step(action=action_step.action)\n",
        "            episode_return += np.mean(time_step_.reward)\n",
        "        total_return += episode_return\n",
        "    average_return = total_return / num_episodes\n",
        "    return average_return\n",
        "\n",
        "# evaluate the agent's policy once before training\n",
        "avg_return = compute_average_return(\n",
        "    env=eval_tf_env,\n",
        "    policy=agent.policy,\n",
        "    num_episodes=num_eval_episodes)\n",
        "print(f\"INFO: Evaluated the agent's policy once before the training, average return: {avg_return:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "VOzsphKClvJR",
        "outputId": "05523ec4-6e9b-446a-812f-208af374c606"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-250c8a8f635f>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# evaluate the agent's policy once before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m avg_return = compute_average_return(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_tf_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-250c8a8f635f>\u001b[0m in \u001b[0;36mcompute_average_return\u001b[0;34m(env, policy, num_episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtime_step_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtotal_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepisode_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/tf_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \"\"\"\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;34m'but saw action with shape %s:\\n   %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 (self.batch_size, action.shape, action))\n\u001b[0;32m--> 315\u001b[0;31m       outputs = tf.numpy_function(\n\u001b[0m\u001b[1;32m    316\u001b[0m           \u001b[0m_isolated_step_py\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m           \u001b[0mflat_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36mnumpy_function\u001b[0;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0mSingle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpy_func_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36mpy_func_common\u001b[0;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \"\"\"\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_isolated_step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_isolated_step_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_step_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    296\u001b[0m         packed = tf.nest.pack_sequence_as(\n\u001b[1;32m    297\u001b[0m             structure=self.action_spec(), flat_sequence=flattened_actions)\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/batched_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_envs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch_nested_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m       \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/machine_learning_rl_training/src/rl_environment_single_agent.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# simulate next lap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RaceReinftrain__simulate_lap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# positions lost or won during pit stop entrance and end of the lap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/racesim/src/race_reinftrain.py\u001b[0m in \u001b[0;36m__simulate_lap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# if VSE (virtual strategy engineer) is used it has to take the strategy decisions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Race__handle_vse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__simulate_lap_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/racesim/src/race.py\u001b[0m in \u001b[0;36m__handle_vse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;31m# take tirechange decisions (are set None for retired drivers) (important: the decisions are taken based on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;31m# the data at the end of the previous lap (with some exceptions, e.g. FCY status))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mnext_compound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m                 decide_pitstop(driver_initials=[driver.initials for driver in self.drivers_list],\n\u001b[1;32m    979\u001b[0m                                \u001b[0mcur_compounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtireset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompound\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrivers_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/racesim/src/vse.py\u001b[0m in \u001b[0;36mdecide_pitstop\u001b[0;34m(self, driver_initials, cur_compounds, no_past_tirechanges, tire_ages, positions_prevlap, pit_prevlap, cur_lap, tot_no_laps, fcy_types, fcy_start_end_progs, bool_driving, bool_driving_prevlap, racetimes_prevlap, location, used_2compounds, cur_positions, cur_racetimes_tmp, t_pit_tirechange_min, t_pit_tirechange_adds, t_pitdrive_inlap, t_pitdrive_outlap, t_pitdrive_inlap_fcy, t_pitdrive_outlap_fcy, t_pitdrive_inlap_sc, t_pitdrive_outlap_sc)\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0mahead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0midx_driver_behind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_positions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcur_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_driver\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbool_driving\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_driver_behind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the agent"
      ],
      "metadata": {
        "id": "V53cwz97fNsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# reset training step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "def train(num_iterations: int = 1):\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  collect_driver.run()\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, _ = next(iterator)\n",
        "  train_loss = agent.train(experience)\n",
        "\n",
        "  iteration = agent.train_step_counter.numpy()\n",
        "  step = int(agent.train_step_counter.numpy())\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "      print(f\"INFO: Step: {step}, loss: {train_loss:.3f}\")\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "      avg_return = compute_average_return(env=eval_tf_env, policy=eval_policy, num_episodes=num_eval_episodes)\n",
        "      print(f\"INFO: Step: {step}, average return: {avg_return:.3f}\")\n",
        "\n",
        "  if (10 * step) % num_iterations == 0:\n",
        "      # print every 10%\n",
        "      print(f\"INFO: Training progress: {step / num_iterations * 100.0:0f}%...\")\n",
        "\n",
        "\n",
        "# train one iteration\n",
        "train(num_iterations=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W28I1imVfPuZ",
        "outputId": "f6203f8b-b45a-497d-f3da-9f29f45e3cf3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training progress: 100%...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup checkpointing and saving."
      ],
      "metadata": {
        "id": "GjWrQjdmGG15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.policies.policy_saver import PolicySaver\n",
        "from tf_agents.utils.common import Checkpointer\n",
        "\n",
        "checkpoint_dir = os.path.join(output_path, \"checkpoint\")\n",
        "policy_dir = os.path.join(output_path, \"policy\")\n",
        "\n",
        "train_checkpointer = Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")\n",
        "policy_saver = PolicySaver(agent.policy)\n",
        "\n",
        "# train_checkpointer.save(global_step)\n",
        "# policy_saver.save(policy_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPU8cAkeFqxK",
        "outputId": "bc23ef82-96e3-4777-caf7-b6544ddb54a1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:497: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train one iteration"
      ],
      "metadata": {
        "id": "XwAwMJRrqoaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(num_iterations=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9hXuDfGLmns",
        "outputId": "c7f7f4e0-93e2-4221-a054-267f0a6521c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training progress: 200%...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save to checkpoint and save policy"
      ],
      "metadata": {
        "id": "PvT0sRxCqviy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_checkpointer.save(global_step)\n",
        "policy_saver.save(policy_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImFem-huqsXR",
        "outputId": "bcfae06b-bfd5-478b-ad6e-e1e0e866246c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:497: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restore checkpoint and load the saved policy."
      ],
      "metadata": {
        "id": "GvvmJtbjq3Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()\n",
        "saved_policy = tf.saved_model.load(policy_dir)"
      ],
      "metadata": {
        "id": "yF29sT1bq1zM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a zipped file from the checkpoint directory."
      ],
      "metadata": {
        "id": "2XnjvuJZtNsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_checkpointer.save(global_step)\n",
        "exported_checkpoint = export_archive(checkpoint_dir, checkpoint_export_dir)\n",
        "print(f\"Exported checkpoint: {exported_checkpoint}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXFBc-Y9tBTA",
        "outputId": "7a7f2ec0-9c15-4a23-9c87-ecf0c55efe4e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported checkpoint: /content/gdrive/MyDrive/RLF1/exports/checkpoints.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the zip file."
      ],
      "metadata": {
        "id": "_NurPUmVt1lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if files is not None:\n",
        "    files.download(exported_checkpoint) # try again if this fails: https://github.com/googlecolab/colabtools/issues/469"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8WDF7hRqtsm9",
        "outputId": "f6798651-7530-4b19-fd34-94ea3b215c00"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5ea42ad1-f1b7-4728-8bfe-5aa2079e30f8\", \"exports.zip\", 105561)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...Come back later and resume training."
      ],
      "metadata": {
        "id": "w6UhDRjpvJLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import_archive(checkpoint_dir)\n",
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Iymck8yPt5Ia",
        "outputId": "0e2d94ca-c2c0-453f-ab13-c424d6b42252"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6444dadd-c9d2-409e-83dc-bf9058491b77\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6444dadd-c9d2-409e-83dc-bf9058491b77\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, you can either (1) continue training iterations, or (2) generate an artifact to check the performance of the loaded policy, or (3) save the policy.\n",
        "\n",
        "When you save the policy and restore it, you cannot continue with the training, but you can deploy the model."
      ],
      "metadata": {
        "id": "44ZGN9qGxOlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the policy."
      ],
      "metadata": {
        "id": "JJb84BLqxkbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_saver.save(policy_dir)\n",
        "print(f\"Saved policy: f{policy_dir}\")\n",
        "\n",
        "exported_policy = export_archive(policy_dir, export_policy_path)\n",
        "print(f\"Exported policy: {exported_policy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq-N8B2Lwl08",
        "outputId": "9c1f1f88-5ce9-4e44-937a-a517b9eb2eaf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_fn, QNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:497: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved policy: f/content/gdrive/MyDrive/RLF1/policy\n",
            "Exported policy: /content/gdrive/MyDrive/RLF1/exports/policies.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download policy."
      ],
      "metadata": {
        "id": "L_jTqDzB1Dmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if files is not None:\n",
        "    files.download(exported_policy) # try again if this fails: https://github.com/googlecolab/colabtools/issues/469"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "h7wq2iYwxraW",
        "outputId": "46c96804-5472-468a-e61d-44d84b5ef9bc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79b26a83-262f-4d1b-9717-9942c037fadc\", \"policies.zip\", 39494)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the policy and check how saved policy performs."
      ],
      "metadata": {
        "id": "VMQiko7F1bFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import_archive(policy_dir)\n",
        "saved_policy = tf.saved_model.load(policy_dir)\n",
        "# run_episodes(saved_policy, eval_env, eval_py_env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "n37sS2wl1Tev",
        "outputId": "63975161-6fec-416d-a249-0cf692af0fc5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1aa7a761-62d3-4855-93d7-956f8c3b7878\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1aa7a761-62d3-4855-93d7-956f8c3b7878\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert policy to TFLite\n",
        "\n",
        "See [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) for more details."
      ],
      "metadata": {
        "id": "BWok1Bac8UE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(policy_dir, signature_keys=[\"action\"])\n",
        "tflite_policy = converter.convert()\n",
        "\n",
        "export_policy_tflite_file = policy_tflite_export_dir / \"policy.tflite\"\n",
        "with open(export_policy_tflite_file, \"wb\") as f:\n",
        "    f.write(tflite_policy)"
      ],
      "metadata": {
        "id": "Jk_2zsAu8Twc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load TFLite model and run inference."
      ],
      "metadata": {
        "id": "P9Jsd-n59mK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "interpreter = tf.lite.Interpreter(export_policy_tflite_file)\n",
        "\n",
        "policy_runner = interpreter.get_signature_runner()\n",
        "print(policy_runner._inputs)\n",
        "\n",
        "policy_runner(**{\n",
        "    \"0/discount\": tf.constant(0.0),\n",
        "    \"0/observation\": tf.zeros([1,4]),\n",
        "    \"0/reward\": tf.constant(0.0),\n",
        "    \"0/step_type\": tf.constant(0),\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "8DUy_tCX9lI2",
        "outputId": "0bb24a88-6ddc-4fa4-fe56-0cd4a71a5284"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f023a1341a8d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_policy_tflite_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpolicy_runner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_signature_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    447\u001b[0m       ]\n\u001b[1;32m    448\u001b[0m       self._interpreter = (\n\u001b[0;32m--> 449\u001b[0;31m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[0m\u001b[1;32m    450\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n",
            "\u001b[0;31mTypeError\u001b[0m: CreateWrapperFromFile(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: str, arg1: int, arg2: List[str], arg3: bool) -> tflite::interpreter_wrapper::InterpreterWrapper\n    2. (arg0: str, arg1: int, arg2: List[str], arg3: List[Callable[[int], None]], arg4: bool) -> tflite::interpreter_wrapper::InterpreterWrapper\n\nInvoked with: PosixPath('/content/gdrive/MyDrive/RLF1/exports/policy-tflite/policy.tflite'), 1, [], [], False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sl6QIoEh-QEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}